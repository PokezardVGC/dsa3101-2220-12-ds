{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# View your Google Chrome browser version: chrome://settings/help\n",
    "# Download ChromeDriver that corresponds to your Google Chrome browser version: https://sites.google.com/chromium.org/driver/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROME_DRIVER_PATH = \"/usr/local/bin/chromedriver_mac_arm64/chromedriver\"\n",
    "DATA_WRITE_PATH = \"/Users/micolechan/Desktop/dsa3101/dsa3101-2220-12-ds/Backend/Data/jobs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = Service(CHROME_DRIVER_PATH)\n",
    "op = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=ser, options=op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(query):\n",
    "    start_time = time.time()\n",
    "    all_jobs_links = []\n",
    "\n",
    "    for page_num in range(0, 100):\n",
    "        try:\n",
    "            driver.get(f'https://www.mycareersfuture.gov.sg/search?search={query}&sortBy=relevancy&page={page_num}')\n",
    "            urls = driver.find_elements(By.XPATH, \"//a[@data-testid='job-card-link']\")\n",
    "            if not urls:\n",
    "                # print(\"End of Search.\")\n",
    "                break\n",
    "            all_jobs_links.extend([i.get_attribute(\"href\") for i in urls])\n",
    "            # print(f\"Scraping of Page {page_num} is completed.\")\n",
    "            \n",
    "        except:\n",
    "            print(\"FAILED\")\n",
    "            driver.quit()\n",
    "            break\n",
    "                \n",
    "    print(f\"Job completed in {round(time.time() - start_time)} seconds\")\n",
    "    print(f\"{len(all_jobs_links)} job listings found\")\n",
    "\n",
    "    return all_jobs_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(list_of_urls):\n",
    "\n",
    "    job_titles = []\n",
    "    job_employers = []\n",
    "    job_descriptions = []\n",
    "    job_urls = []\n",
    "    count = 0\n",
    "\n",
    "    for i in tqdm.trange(len(list_of_urls)):\n",
    "        url = list_of_urls[i]\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            job_title_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"job_title\"))\n",
    "            )\n",
    "            job_title = job_title_element.text\n",
    "            company = driver.find_element(By.CSS_SELECTOR, \"p[data-cy='company-hire-info__company']\").text\n",
    "            job_description_element = driver.find_element(By.ID, \"job_description\")\n",
    "            job_description = job_description_element.find_element(By.ID, \"description-content\").text\n",
    "            job_titles.append(job_title)\n",
    "            job_employers.append(company)\n",
    "            job_descriptions.append(job_description)\n",
    "            job_urls.append(url)\n",
    "        except Exception as e :\n",
    "            count += 1 \n",
    "            print(e)\n",
    "            print(url)\n",
    "            print(count)\n",
    "    jobs_df = pd.DataFrame([job_titles, job_employers, job_descriptions, job_urls]).\\\n",
    "                transpose().\\\n",
    "                rename(columns = {0 : \"Title\",\n",
    "                                  1 : \"Company\",\n",
    "                                  2 : \"Description\",\n",
    "                                  3 : \"URL\"})\n",
    "    return jobs_df\n",
    "    \n",
    "# scraper(all_jobs_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(save_file_name, list_of_urls):\n",
    "    with open(DATA_WRITE_PATH + f'mycareersfuture_job_url_query-{save_file_name}.txt', 'w') as f:\n",
    "        for url in list_of_urls:\n",
    "            f.write(url)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(DATA_WRITE_PATH + f'mycareersfuture_job_url_query-{save_file_name}.txt') as file:\n",
    "        urls = [line.rstrip() for line in file]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['\"data analyst\"','\"data engineer\"', '\"data science\"', '\"machine learning engineer\"']\n",
    "\n",
    "file_names = [query.replace('\"', '').replace(' ', '_') for query in queries]\n",
    "\n",
    "for index in range(len(queries)):\n",
    "    driver = webdriver.Chrome(service=ser, options=op)\n",
    "    print(f\"Scraping website for {queries[index]} roles.\")\n",
    "    \n",
    "    # 1. Get list of urls \n",
    "    list_of_urls = get_urls(queries[index])\n",
    "    \n",
    "    # 2. Save result as txt file\n",
    "    write_to_file(file_names[index], list_of_urls)\n",
    "    print(f\".txt file created with list of URLS for {queries[index]}\")\n",
    "    \n",
    "    # 3. Read URL text \n",
    "    formatted_urls = read_file(file_names[index])\n",
    "    df = scraper(formatted_urls)\n",
    "    df.to_csv(DATA_WRITE_PATH + f\"mycareersfuture_query-{file_names[index]}.csv\")\n",
    "    print(f\".csv file created for {queries[index]}\")\n",
    "    \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FOR ONE QUERY \n",
    "# query = '\"data analyst\"'\n",
    "# save_file_name = \"data_analyst\"\n",
    "\n",
    "# list_of_urls = get_urls(query)\n",
    "# write_to_file(save_file_name, list_of_urls)\n",
    "# print(f\".txt file cretaed with list of URLS for {query}\")\n",
    "# formatted_urls = read_file(save_file_name)\n",
    "# df = scraper(formatted_urls)\n",
    "# df.to_csv(DATA_WRITE_PATH + f\"mycareersfuture_query-{save_file_name}.csv\")\n",
    "# print(f\".csv file created for {query}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
