{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use:\n",
    "1. Fill in your query in the first cell\n",
    "2. Put in the path to your web driver in the second cell.\n",
    "3. If you have not installed the web driver, follow the instructions in the third cell\n",
    "4. Run the remaining cells in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['\"data analyst\"','\"data engineer\"', '\"data science\"', '\"machine learning engineer\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the ChromeDriver executable\n",
    "CHROME_DRIVER_PATH = \"/usr/local/bin/chromedriver_mac_arm64/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# View your Google Chrome browser version: chrome://settings/help\n",
    "# Download ChromeDriver that corresponds to your Google Chrome browser version: https://sites.google.com/chromium.org/driver/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory to save the output CSV files\n",
    "DATA_WRITE_PATH = \"./../../Data/jobs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising parameters for Chrome Driver\n",
    "ser = Service(CHROME_DRIVER_PATH)\n",
    "op = webdriver.ChromeOptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving URLs to all search results from a particular query\n",
    "Input: Query to input into search engine on https://www.mycareersfuture.gov.sg/ \n",
    "\n",
    "Output: List of all URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(query):\n",
    "    start_time = time.time()\n",
    "    # List to store the URLs of job listings\n",
    "    all_jobs_links = []\n",
    "\n",
    "    # Loop over the first 100 pages of search results\n",
    "    for page_num in range(0, 1000):\n",
    "        try:\n",
    "            # Navigate to the page for the current search query and page number\n",
    "            driver.get(f'https://www.mycareersfuture.gov.sg/search?search={query}&sortBy=relevancy&page={page_num}')\n",
    "            # Find all the job listing URLs on the page\n",
    "            urls = driver.find_elements(By.XPATH, \"//a[@data-testid='job-card-link']\")\n",
    "            \n",
    "            # If no URLs are found, break out of the loop\n",
    "            if not urls:\n",
    "                break\n",
    "                \n",
    "            # Add the URLs to the list of all job listing URLs\n",
    "            all_jobs_links.extend([i.get_attribute(\"href\") for i in urls])\n",
    "            \n",
    "        # If there is an exception, print \"FAILED\" and quit the driver\n",
    "        except:\n",
    "            print(\"FAILED\")\n",
    "            driver.quit()\n",
    "            break\n",
    "                \n",
    "    print(f\"Job completed in {round(time.time() - start_time)} seconds\")\n",
    "    print(f\"{len(all_jobs_links)} job listings found\")\n",
    "\n",
    "    # Return the list of all job listing URLs\n",
    "    return all_jobs_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Indiviudal Job Postings from each URL\n",
    "Input: List of URLS\n",
    "\n",
    "Output: Dataframe with extracted information:  \n",
    "\n",
    "- job titles, companies, descriptions and URLs of job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(list_of_urls):\n",
    "    \n",
    "    job_titles = []\n",
    "    job_employers = []\n",
    "    job_descriptions = []\n",
    "    job_urls = []\n",
    "    # Initialize a counter to keep track of any exceptions that occur during scraping\n",
    "    count = 0\n",
    "\n",
    "    # Iterate over the list of URLs and scrape job information\n",
    "    for i in tqdm.trange(len(list_of_urls)):\n",
    "        # Get the URL at the current index\n",
    "        url = list_of_urls[i]\n",
    "        try:\n",
    "            # Load the URL and wait for the job title element to appear\n",
    "            driver.get(url)\n",
    "            job_title_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"job_title\"))\n",
    "            )\n",
    "            # Extract the job title, employer, and description elements\n",
    "            job_title = job_title_element.text\n",
    "            company = driver.find_element(By.CSS_SELECTOR, \"p[data-cy='company-hire-info__company']\").text\n",
    "            job_description_element = driver.find_element(By.ID, \"job_description\")\n",
    "            job_description = job_description_element.find_element(By.ID, \"description-content\").text\n",
    "            # Append the scraped job information to the respective lists\n",
    "            job_titles.append(job_title)\n",
    "            job_employers.append(company)\n",
    "            job_descriptions.append(job_description)\n",
    "            job_urls.append(url)\n",
    "        except Exception as e :\n",
    "            # If an exception occurs, increment the counter and print information about the error\n",
    "            count += 1 \n",
    "            print(e)\n",
    "            print(url)\n",
    "            print(count)\n",
    "    # Convert the scraped job information into a pandas dataframe and return it\n",
    "    jobs_df = pd.DataFrame([job_titles, job_employers, job_descriptions, job_urls]).\\\n",
    "                transpose().\\\n",
    "                rename(columns = {0 : \"Title\",\n",
    "                                  1 : \"Company\",\n",
    "                                  2 : \"Description\",\n",
    "                                  3 : \"URL\"})\n",
    "    return jobs_df\n",
    "    \n",
    "# scraper(all_jobs_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(save_file_name, list_of_urls):\n",
    "    with open(DATA_WRITE_PATH + f'mycareersfuture_job_url_query-{save_file_name}.txt', 'w') as f:\n",
    "        for url in list_of_urls:\n",
    "            f.write(url)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(DATA_WRITE_PATH + f'mycareersfuture_job_url_query-{file_name}.txt') as file:\n",
    "        urls = [line.rstrip() for line in file]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [query.replace('\"', '').replace(' ', '_') for query in queries]\n",
    "\n",
    "for index in range(len(queries)):\n",
    "    driver = webdriver.Chrome(service=ser, options=op)\n",
    "    print(f\"Scraping website for {queries[index]} roles.\")\n",
    "    \n",
    "    # 1. Get list of urls \n",
    "    list_of_urls = get_urls(queries[index])\n",
    "    \n",
    "    # 2. Save result as txt file\n",
    "    write_to_file(file_names[index], list_of_urls)\n",
    "    print(f\".txt file created with list of URLS for {queries[index]}\")\n",
    "    \n",
    "    # 3. Read URL text \n",
    "    formatted_urls = read_file(file_names[index])\n",
    "\n",
    "     # 4. Scrape job information from URLs and create a DataFrame\n",
    "    df = scraper(formatted_urls)\n",
    "\n",
    "    # 5. Save DataFrame as a csv file\n",
    "    df.to_csv(DATA_WRITE_PATH + f\"mycareersfuture_query-{file_names[index]}.csv\")\n",
    "    print(f\".csv file created for {queries[index]}\")\n",
    "    \n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "388bcaf41e3432fc6dd5fbce2e83eae71b1cc1a2efb1b36d40bd4c0c9dd3c24f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
