{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micolechan/opt/anaconda3/envs/cs4243/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "\talgorithm\n",
      "\tstatist\n",
      "\tproblem\n",
      "\tdistribut\n",
      "\tmethod\n",
      "\tcomput\n",
      "\tanalysi\n",
      "\tsearch\n",
      "\tuse\n",
      "\tmodel\n",
      "\n",
      "Topic 1:\n",
      "\tlearn\n",
      "\tcomput\n",
      "\tproblem\n",
      "\tdata\n",
      "\tprogram\n",
      "\tmachin\n",
      "\tbasic\n",
      "\tdesign\n",
      "\tuse\n",
      "\tmodel\n",
      "\n",
      "Topic 2:\n",
      "\tdata\n",
      "\tsystem\n",
      "\tuse\n",
      "\tdesign\n",
      "\tsoftwar\n",
      "\tlearn\n",
      "\tapplic\n",
      "\tnetwork\n",
      "\tmodel\n",
      "\tproblem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def drop_missing_descriptions(df):\n",
    "    \"\"\"\n",
    "    Drop rows with missing module descriptions.\n",
    "    \"\"\"\n",
    "    df.dropna(subset=['All Module Details'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stop words, stem, and lemmatize the text.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stop words from the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # add more stop words that are not useful for your analysis\n",
    "    additional_stop_words = set(['module', 'modules', 'course', 'courses', 'level', \"aims\", \"aim\", \"covers\", \"essential\", \"equip\", \"students\", \"provide\", \"understanding\",\"develop\"])\n",
    "    stop_words = stop_words | additional_stop_words\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Perform stemming and lemmatization on each token\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "    \n",
    "    # Join the stemmed and lemmatized tokens back into a single text string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def preprocess_descriptions(df):\n",
    "    \"\"\"\n",
    "    Apply the text processing function to the module descriptions.\n",
    "    \"\"\"\n",
    "    df['All Module Details'] = df['All Module Details'].apply(process_text)\n",
    "    return df\n",
    "\n",
    "def generate_document_term_matrix(df):\n",
    "    \"\"\"\n",
    "    Generate a document-term matrix for the preprocessed descriptions.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df['All Module Details'])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return X, feature_names\n",
    "\n",
    "def apply_lda(X, feature_names, num_topics=5):\n",
    "    \"\"\"\n",
    "    Apply LDA to the document-term matrix to find topics.\n",
    "    \"\"\"\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "    lda.fit(X)\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        top_feature_indices = topic.argsort()[:-11:-1]\n",
    "        for feature_index in top_feature_indices:\n",
    "            print(f\"\\t{feature_names[feature_index]}\")\n",
    "        print(\"\")\n",
    "\n",
    "def find_topics(df, num_topics=3):\n",
    "    \"\"\"\n",
    "    Find topics in the dataframe using LDA.\n",
    "    \"\"\"\n",
    "    df = drop_missing_descriptions(df)\n",
    "    df = preprocess_descriptions(df)\n",
    "    X, feature_names = generate_document_term_matrix(df)\n",
    "    apply_lda(X, feature_names, num_topics)\n",
    "\n",
    "def merge_data(selected_columns):\n",
    "    \"\"\"\n",
    "    Load dataframes from files and select only desired columns.\n",
    "    \"\"\"\n",
    "    ntu_df = pd.read_excel(\"../../Backend/Data/university_courses_graph/NTU_course_info.xlsx\")\n",
    "    sit_df = pd.read_csv(\"../../Backend/Data/university_courses_graph/SIT_Module_Info.csv\")\n",
    "    nus_df = pd.read_csv(\"../../Backend/Data/university_courses_graph/nus_dsa_mods.csv\")\n",
    "    smu_df = pd.read_csv(\"../../Backend/Data/university_courses_graph/SMU_course_info.csv\")\n",
    "    suss_df = pd.read_csv(\"../../Backend/Data/university_courses_graph/SUSS_course_info.csv\")\n",
    "    sutd_df = pd.read_csv(\"../../Backend/Data/university_courses_graph/SUTD_course_info.csv\")\n",
    "\n",
    "    # Select only the desired columns\n",
    "    ntu_selected = ntu_df.loc[:, selected_columns]\n",
    "    sit_selected = sit_df.loc[:, selected_columns]\n",
    "    nus_selected = nus_df.loc[:, selected_columns]\n",
    "    smu_selected = smu_df.loc[:, selected_columns]\n",
    "    suss_selected = suss_df.loc[:, selected_columns]\n",
    "    sutd_selected = sutd_df.loc[:, selected_columns]\n",
    "\n",
    "    # Concatenate the selected dataframes\n",
    "    merged_df = pd.concat([ntu_selected, sit_selected, nus_selected, smu_selected, suss_selected, sutd_selected], axis=0)\n",
    "\n",
    "    # Reset the index of the merged dataframe\n",
    "    df = merged_df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "selected_columns = [\"University\", \"Module Title\", \"Module Code\", \"All Module Details\", \"Module Level\"]\n",
    "df = merge_data(selected_columns)\n",
    "find_topics(df)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "388bcaf41e3432fc6dd5fbce2e83eae71b1cc1a2efb1b36d40bd4c0c9dd3c24f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 ('cs4243')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
