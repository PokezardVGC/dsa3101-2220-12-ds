Query,Skill,Skill_Description,start_ind,end_ind
data,Data Entry,"Data entry, a person-based process, is ""one of the important basic"" tasks needed when no machine-readable version of the information for planned computer-based analysis or processing is readily available.",[0],[10]
data,Data Analysis,"Data analysis is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.","[0, 189, 396]","[13, 202, 409]"
data,Data Collection,"Data collection is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities, and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture quality evidence that allows analysis to lead to the formulation of convincing and credible answers to the questions that have been posed.Data collection and validation consists of four steps when it involves taking a census and seven steps when it involves sampling","[0, 193, 445, 613]","[15, 208, 460, 628]"
data,Data Management,Data Management comprises all disciplines related to managing data as a valuable resource.,[0],[15]
data,Relational Databases,"A relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970.
A software system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems have an option of using the SQL for querying and maintaining the database.",[153],[173]
data,Data Modeling,Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques.,[0],[13]
data,Data Integrity,"Data integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle and is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context – even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation is a pre-requisite for data integrity.
Data integrity is the opposite of data corruption. The overall intent of any data integrity technique is the same: ensure data is recorded exactly as intended. Moreover, upon later retrieval, ensure the data is the same as when it was originally recorded. In short, data integrity aims to prevent unintentional changes to information. Data integrity is not to be confused with data security, the discipline of protecting data from unauthorized parties.","[0, 492, 508, 585, 774, 843]","[14, 506, 522, 599, 788, 857]"
data,Big Data,"Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (columns) offer greater statistical power, while data with higher complexity may lead to a higher false discovery rate. Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Therefore, big data often includes data with sizes that exceed the capacity of traditional software to process within an acceptable time and value.","[0, 365, 547, 654, 772]","[8, 373, 555, 662, 780]"
data,Database Administration,"Database administration is the function of managing and maintaining database management systems (DBMS) software. Mainstream DBMS software such as Oracle, IBM DB2 and Microsoft SQL Server need ongoing management. As such, corporations that use DBMS software often hire specialized information technology personnel called database administrators or DBAs.",[0],[23]
data,Data Quality,"Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is ""fit for [its] intended uses in operations, decision making and planning"". Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.","[0, 117, 606, 815, 922]","[12, 129, 618, 827, 934]"
data,Data Science,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.","[0, 279]","[12, 291]"
data,Data Processing,"Data processing is, generally, ""the collection and manipulation of items of data to produce meaningful information."" 
In this sense it can be considered a subset of information processing, ""the change (processing) of information in any manner detectable by an observer."" 
",[0],[15]
data,Data Mining,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.","[0, 176, 395]","[11, 187, 406]"
data,Data Visualization,"Data visualization is an interdisciplinary field that deals with the graphic representation of data. It is a particularly efficient way of communicating when the data is numerous as for example a Time Series. From an academic point of view, this representation can be considered as a mapping between the original data and graphic elements. The mapping determines how the attributes of these elements vary according to the data. In this light, a bar chart is a mapping of the length of a bar to a magnitude of a variable. Since the graphic design of the mapping can adversely affect the readability of a chart, mapping is a core competency of Data visualization. Data visualization has its roots in the field of Statistics and is therefore generally considered a branch of Descriptive Statistics. However, because both design skills and statistical and computing skills are required to visualize effectively, it is argued by some authors that it is both an Art and a Science.","[0, 642, 662]","[18, 660, 680]"
data,Database Design,"Database design is the organization of data according to a database model. The designer determines what data must be stored and how the data elements interrelate. With this information, they can begin to fit the data to the database model.
Database management system manages the data accordingly.",[0],[15]
data,Customer Data Management,"Customer data management (CDM) is the ways in which businesses keep track of their customer information and survey their customer base in order to obtain feedback. CDM embraces a range of software or cloud computing applications designed to give large organizations rapid and efficient access to customer data. Surveys and data can be centrally located and widely accessible within a company, as opposed to being warehoused in separate departments. CDM encompasses the collection, analysis, organizing, reporting and sharing of customer information throughout an organization. Businesses need a thorough understanding of their customers’ needs if they are to retain and increase their customer base. Efficient CDM solutions provide companies with the ability to deal instantly with customer issues and obtain immediate feedback. As a result, customer retention and customer satisfaction can show marked improvement. According to a study by Aberdeen Group inc.: ""Above-average and best-in-class companies... attain greater than 20% annual improvement in retention rates, revenues, data accuracy and partner/customer satisfaction rates.""",[0],[24]
data,Test Data,"Test data is data which has been specifically identified for use in tests, typically of a computer program.",[0],[9]
data,Data Reporting,"Data reporting is the process of collecting and submitting data which gives rise to accurate analyses of the facts on the ground; inaccurate data reporting can lead to vastly uninformed decision-making based on erroneous evidence. Different from data analysis that transforms data and information into insights, data reporting is the previous step that translates raw data into information. When data is not reported, the problem is known as underreporting; the opposite problem leads to false positives.","[0, 141, 312]","[14, 155, 326]"
data,Data Integration,"Data integration involves combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial and scientific domains. Data integration appears with increasing frequency as the volume and the need to share existing data explodes. It has become the focus of extensive theoretical work, and numerous open problems remain unsolved. Data integration encourages collaboration between internal as well as external users. The data being integrated must be received from a heterogeneous database system and transformed to a single coherent data store that provides synchronous data across a network of files for clients. A common use of data integration is in data mining when analyzing and extracting information from existing databases that can be useful for Business information.","[0, 235, 445, 745]","[16, 251, 461, 761]"
data,Data Engineering,"Information engineering (IE), also known as Information technology engineering (ITE), information engineering methodology (IEM) or data engineering, is a software engineering approach to designing and developing information systems.",[131],[147]
data,Data Architecture,"In information technology, data architecture is composed of models, policies, rules or standards that govern which data is collected, and how it is stored, arranged, integrated, and put to use in data systems and in organizations. Data is usually one of several architecture domains that form the pillars of an enterprise architecture or solution architecture.",[27],[44]
data,Data Governance,Data governance is a term used on both a macro and a micro level. The former is a political concept and forms part of international relations and Internet governance; the latter is a data management concept and forms part of corporate data governance.,"[0, 235]","[15, 250]"
data,Material Safety Data Sheet,"A safety data sheet (SDS), material safety data sheet (MSDS), or product safety data sheet (PSDS) is a document that lists information relating to occupational safety and health for the use of various substances and products. SDSs are a widely used system for cataloguing information on chemicals, chemical compounds, and chemical mixtures. SDS information may include instructions for the safe use and potential hazards associated with a particular material or product, along with spill-handling procedures. The older MSDS formats could vary from source to source within a country depending on national requirements; however, the newer SDS format is internationally standardized.",[27],[53]
data,Data Acquisition,"Data acquisition is the process of sampling signals that measure real world physical conditions and converting the resulting samples into digital numeric values that can be manipulated by a computer. Data acquisition systems, abbreviated by the initialisms DAS, DAQ, or DAU, typically convert analog waveforms into digital values for processing. The components of data acquisition systems include:Sensors, to convert physical parameters to electrical signals.
Signal conditioning circuitry, to convert sensor signals into a form that can be converted to digital values.
Analog-to-digital converters, to convert conditioned sensor signals to digital values.","[0, 200, 364]","[16, 216, 380]"
data,Database Application,"A database application is a computer program whose primary purpose is entering and retrieving information from a computerized database. Early examples of database applications were accounting systems and airline reservations systems, such as SABRE, developed starting in 1957.","[2, 154]","[22, 174]"
data,Data Migration,"Data migration is the process of selecting, preparing, extracting, and transforming data and permanently transferring it from one computer storage system to another. Additionally, the validation of migrated data for completeness and the decommissioning of legacy data storage are considered part of the entire data migration process. Data migration is a key consideration for any system implementation, upgrade, or consolidation, and it is typically performed in such a way as to be as automated as possible, freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, application migration, website consolidation, disaster recovery, and data center relocation.","[0, 310, 334, 556]","[14, 324, 348, 570]"
data,Data Manipulation,"A data manipulation language (DML) is a computer programming language used for adding (inserting), deleting, and modifying (updating) data in a database. A DML is often a sublanguage of a broader database language such as SQL, with the DML comprising some of the operators in the language. Read-only selecting of data is sometimes distinguished as being part of a separate data query language (DQL), but it is closely related and sometimes also considered a component of a DML; some operators may perform both selecting (reading) and writing.",[2],[19]
data,Dataflow,"In computing, dataflow is a broad concept, which has various meanings depending on the application and context. In the context of software architecture, data flow relates to stream processing or reactive programming.",[14],[22]
data,Operational Data Store,"An operational data store is used for operational reporting and as a source of data for the Enterprise Data Warehouse (EDW). It is a complementary element to an EDW in a decision support landscape, and is used for operational reporting, controls and decision making, as opposed to the EDW, which is used for tactical and strategic decision support.",[3],[25]
data,Data Validation,"In computer science, data validation is the process of ensuring data has undergone data cleansing to ensure they have, that is, that they are both correct and useful. It uses routines, often called ""validation rules"", ""validation constraints"", or ""check routines"", that check for correctness, meaningfulness, and security of data that are input to the system. The rules may be implemented through the automated facilities of a data dictionary, or by the inclusion of explicit application program validation logic of the computer and its application.",[21],[36]
data,Data Extraction,Data extraction is the act or process of retrieving data out of data sources for further data processing or data storage. The import into the intermediate extracting system is thus usually followed by data transformation and possibly the addition of metadata prior to export to another stage in the data workflow.,[0],[15]
data,Java Database Connectivity,"Java Database Connectivity (JDBC) is an application programming interface (API) for the programming language Java, which defines how a client may access a database. It is a Java-based data access technology used for Java database connectivity. It is part of the Java Standard Edition platform, from Oracle Corporation. It provides methods to query and update data in a database, and is oriented toward relational databases. A JDBC-to-ODBC bridge enables connections to any ODBC-accessible data source in the Java virtual machine (JVM) host environment.","[0, 216]","[26, 242]"
data,Data Mapping,"In computing and data management, data mapping is the process of creating data element mappings between two distinct data models. Data mapping is used as a first step for a wide variety of data integration tasks, including:Data transformation or data mediation between a data source and a destination
Identification of data relationships as part of data lineage analysis
Discovery of hidden sensitive data such as the last four digits of a social security number hidden in another user id as part of a data masking or de-identification project
Consolidation of multiple databases into a single database and identifying redundant columns of data for consolidation or elimination","[34, 130]","[46, 142]"
data,Operational Databases,"Operational database management systems, are used to update data in real-time. These types of databases allow users to do more than simply view archived data. Operational databases allow you to modify that data, doing it in real-time. OLTP databases provide transactions as main abstraction to guarantee data consistency that guarantee the so-called ACID properties. Basically, the consistency of the data is guaranteed in the case of failures and/or concurrent access to the data.",[159],[180]
data,Data Access,"Data access is a generic term referring to a process which has both an IT-specific meaning and other connotations involving access rights in a broader legal and/or political sense. In the former it typically refers to software and activities related to storing, retrieving, or acting on data housed in a database or other repository.",[0],[11]
data,Data Conversion,"Data conversion is the conversion of computer data from one format to another. Throughout a computer environment, data is encoded in a variety of ways. For example, computer hardware is built on the basis of certain standards, which requires that data contains, for example, parity bit checks. Similarly, the operating system is predicated on certain standards for data and file handling. Furthermore, each computer program handles data in a different manner. Whenever any one of these variables is changed, data must be converted in some way before it can be used by a different computer, operating system or program. Even different versions of these elements usually involve different data structures. For example, the changing of bits from one format to another, usually for the purpose of application interoperability or of capability of using new features, is merely a data conversion. Data conversions may be as simple as the conversion of a text file from one character encoding system to another; or more complex, such as the conversion of office file formats, or the conversion of image formats and audio file formats.","[0, 874, 891]","[15, 889, 906]"
data,Data Store,"A data store is a repository for persistently storing and managing collections of data which include not just repositories like databases, but also simpler store types such as simple files, emails etc.",[2],[12]
data,Data Cleansing,"Data cleansing or data cleaning is the process of detecting and correcting corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.","[0, 304]","[14, 318]"
data,Unstructured Data,"Unstructured data is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in fielded form in databases or annotated in documents.",[0],[17]
data,Electronic Data Interchange,"Electronic data interchange (EDI) is the concept of businesses electronically communicating information that was traditionally communicated on paper, such as purchase orders and invoices. Technical standards for EDI exist to facilitate parties transacting such instruments without having to make special arrangements.",[0],[27]
data,Supervisory Control And Data Acquisition (SCADA),"Supervisory control and data acquisition (SCADA) is a control system architecture comprising computers, networked data communications and graphical user interfaces (GUI) for high-level process supervisory management, while also comprising other peripheral devices like programmable logic controllers (PLC) and discrete proportional-integral-derivative (PID) controllers to interface with process plant or machinery. The use of SCADA has been considered also for management and operations of project-driven-process in construction.",[0],[48]
data,Data Capture,"Automatic identification and data capture (AIDC) refers to the methods of automatically identifying objects, collecting data about them, and entering them directly into computer systems, without human involvement. Technologies typically considered as part of AIDC include QR codes, bar codes, radio frequency identification (RFID), biometrics, magnetic stripes, optical character recognition (OCR), smart cards, and voice recognition. AIDC is also commonly referred to as ""Automatic Identification"", ""Auto-ID"" and ""Automatic Data Capture"".","[29, 525]","[41, 537]"
data,Market Data,"In finance, market data is price and trade-related data for a financial instrument reported by a trading venue such as a stock exchange. Market data allows traders and investors to know the latest price and see historical trends for instruments such as equities, fixed-income products, derivatives, and currencies.","[12, 137]","[23, 148]"
data,Data Dictionary,"
A data dictionary, or metadata repository, as defined in the IBM Dictionary of Computing, is a ""centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format"". Oracle defines it as a collection of tables with metadata. The term can have one of several closely related meanings pertaining to databases and database management systems (DBMS):A document describing a database or collection of databases
An integral component of a DBMS that is required to determine its structure
A piece of middleware that extends or supplants the native data dictionary of a DBMS","[3, 595]","[18, 610]"
data,Data Transformation,"In statistics, data transformation is the application of a deterministic mathematical function to each point in a data set—that is, each data point zi is replaced with the transformed value yi = f(zi), where f is a function. Transforms are usually applied so that the data appear to more closely meet the assumptions of a statistical inference procedure that is to be applied, or to improve the interpretability or appearance of graphs.",[15],[34]
data,Data Profiling,"Data profiling is the process of examining the data available from an existing information source and collecting statistics or informative summaries about that data. The purpose of these statistics may be to:Find out whether existing data can be easily used for other purposes
Improve the ability to search data by tagging it with keywords, descriptions, or assigning it to a category
Assess data quality, including whether the data conforms to particular standards or patterns
Assess the risk involved in integrating data in new applications, including the challenges of joins
Discover metadata of the source database, including value patterns and distributions, key candidates, foreign-key candidates, and functional dependencies
Assess whether known metadata accurately describes the actual values in the source database
Understanding data challenges early in any data intensive project, so that late project surprises are avoided. Finding data problems late in the project can lead to delays and cost overruns.
Have an enterprise view of all data, for uses such as master data management, where key data is needed, or data governance for improving data quality.",[0],[14]
data,Data Infrastructure,A data infrastructure is a digital infrastructure promoting data sharing and consumption.,[2],[21]
data,Database Security,"Database security  concerns the use of a broad range of information security controls to protect databases against compromises of their confidentiality, integrity and availability. It involves various types or categories of controls, such as technical, procedural/administrative and physical.",[0],[17]
data,Transaction Data,"Transaction data is data describing an event and is usually described with verbs. Transaction data always has a time dimension, a numerical value and refers to one or more objects.","[0, 82]","[16, 98]"
data,Data Loss Prevention,"Data loss prevention software detects potential data breaches/data ex-filtration transmissions and prevents them by monitoring, detecting and blocking sensitive data while in use, in motion, and at rest.",[0],[20]
data,Data Mart,"A data mart is a structure / access pattern specific to data warehouse environments, used to retrieve client-facing data. The data mart is a subset of the data warehouse and is usually oriented to a specific business line or team. Whereas data warehouses have an enterprise-wide depth, the information in data marts pertains to a single department. In some deployments, each department or business unit is considered the owner of its data mart including all the hardware, software and data. This enables each department to isolate the use, manipulation and development of their data. In other deployments where conformed dimensions are used, this business unit ownership will not hold true for shared dimensions like customer, product, etc.","[2, 126, 305, 434]","[11, 135, 314, 443]"
data,Data Sharing,"Data sharing is the practice of making data used for scholarly research available to other investigators. Many funding agencies, institutions, and publication venues have policies regarding data sharing because transparency and openness are considered by many to be part of the scientific method.","[0, 190]","[12, 202]"
data,IBM InfoSphere DataStage,"IBM InfoSphere DataStage is an ETL tool and part of the IBM Information Platforms Solutions suite and IBM InfoSphere. It uses a graphical notation to construct data integration solutions and is available in various versions such as the Server Edition, the Enterprise Edition, and the MVS Edition. It uses a client-server architecture. The servers can be deployed in both Unix as well as Windows.",[0],[24]
data,Data Retrieval,"Data retrieval means obtaining data from a database management system such as ODBMS. In this case, it is considered that data is represented in a structured way, and there is no ambiguity in data.",[0],[14]
data,Reference Data,"Reference data is data used to classify or categorize other data. Typically, they are static or slowly changing over time.",[0],[14]
data,Datasheets,"A data sheet, data-sheet, or spec sheet is a document that summarizes the performance and other characteristics of a product, machine, component, material, subsystem, or software in sufficient detail that allows a buyer to understand what the product is and a design engineer to understand the role of the component in the overall system. Typically, a datasheet is created by the manufacturer and begins with an introductory page describing the rest of the document, followed by listings of specific characteristics, with further information on the connectivity of the devices. In cases where there is relevant source code to include, it is usually attached near the end of the document or separated into another file. Datasheets are created, stored, and distributed via product information management or product data management systems.",[719],[729]
data,Database Tuning,"Database tuning describes a group of activities used to optimize and homogenize the performance of a database. It usually overlaps with query tuning, but refers to design of the database files, selection of the database management system (DBMS) application, and configuration of the database's environment.",[0],[15]
data,Product Data Management,"Product data management (PDM) or Product information management (PIM) is the business function often within product lifecycle management (PLM) that is responsible for the management and publication of product data. In software engineering, this is known as version control. The goals of product data management include ensuring all stakeholders share a common understanding, that confusion during the execution of the processes is minimized, and that the highest standards of quality controls are maintained.","[0, 287]","[23, 310]"
data,Database Marketing,"Database marketing is a form of direct marketing using databases of customers or potential customers to generate personalized communications in order to promote a product or service for marketing purposes. The method of communication can be any addressable medium, as in direct marketing.",[0],[18]
data,Online Databases,"An online database is a database accessible from a local network or the Internet, as opposed to one that is stored locally on an individual computer or its attached storage. Online databases are hosted on websites, made available as software as a service products accessible via a web browser. They may be free or require payment, such as by a monthly subscription. Some have enhanced features such as collaborative editing and email notification.",[174],[190]
data,Electronic Data Capture (EDC),An electronic data capture (EDC) system is a computerized system designed for the collection of clinical data in electronic format for use mainly in human clinical trials. EDC replaces the traditional paper-based data collection methodology to streamline data collection and expedite the time to market for drugs and medical devices. EDC solutions are widely adopted by pharmaceutical companies and contract research organizations (CRO).,[3],[32]
data,Databricks,"Databricks is a company founded by the original creators of Apache Spark. Databricks grew out of the AMPLab project at University of California, Berkeley that was involved in making Apache Spark, an open-source distributed computing framework built atop Scala. Databricks develops a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks. In addition to building the Databricks platform, the company is co-organizing massive open online courses about Spark and runs the largest conference about Spark - Spark Summit.","[0, 74, 261, 426]","[10, 84, 271, 436]"
data,Data Exchange,"Data exchange is the process of taking data structured under a source schema and transforming it into a target schema, so that the target data is an accurate representation of the source data. Data exchange allows data to be shared between different computer programs.","[0, 193]","[13, 206]"
data,Distributed Data Store,"A distributed data store is a computer network where information is stored on more than one node, often in a replicated fashion. It is usually specifically used to refer to either a distributed database where users store information on a number of nodes, or a computer network in which users store information on a number of peer network nodes.",[2],[24]
data,Experimental Data,"Experimental data in science and engineering is data produced by a measurement, test method, experimental design or quasi-experimental design. In clinical research any data produced are the result of a clinical trial. Experimental data may be qualitative or quantitative, each being appropriate for different investigations.","[0, 218]","[17, 235]"
data,Datadog,"Datadog is a monitoring service for cloud-scale applications, providing monitoring of servers, databases, tools, and services, through a SaaS-based data analytics platform.",[0],[7]
data,Database Schema,"The database schema is its structure described in a formal language supported by the database management system (DBMS). The term ""schema"" refers to the organization of data as a blueprint of how the database is constructed. The formal definition of a database schema is a set of formulas (sentences) called integrity constraints imposed on a database. These integrity constraints ensure compatibility between parts of the schema. All constraints are expressible in the same language. A database can be considered a structure in realization of the database language. The states of a created conceptual schema are transformed into an explicit mapping, the database schema. This describes how real-world entities are modeled in the database.","[4, 251, 654]","[19, 266, 669]"
data,Data Verification,Data verification is a process in which different types of data are checked for accuracy and inconsistencies after data migration is done.,[0],[17]
data,Electronic Data Processing,"Electronic data processing (EDP) can refer to the use of automated methods to process commercial data. Typically, this uses relatively simple, repetitive activities to process large volumes of similar information. For example: stock updates applied to an inventory, banking transactions applied to account and customer master files, booking and ticketing transactions to an airline's reservation system, billing for utility services. The modifier ""electronic"" or ""automatic"" was used with ""data processing"" (DP), especially c. 1960, to distinguish human clerical data processing from that done by computer.",[0],[26]
data,Open Database Connectivity,"In computing, Open Database Connectivity (ODBC) is a standard application programming interface (API) for accessing database management systems (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. An application written using ODBC can be ported to other platforms, both on the client and server side, with few changes to the data access code.",[14],[40]
data,Data Recovery,"In computing, data recovery is a process of salvaging (retrieving) inaccessible, lost, corrupted, damaged or formatted data from secondary storage, removable media or files, when the data stored in them cannot be accessed in a usual way. The data is most often salvaged from storage media such as internal or external hard disk drives (HDDs), solid-state drives (SSDs), USB flash drives, magnetic tapes, CDs, DVDs, RAID subsystems, and other electronic devices. Recovery may be required due to physical damage to the storage devices or logical damage to the file system that prevents it from being mounted by the host operating system (OS).",[14],[27]
data,Missing Data,"In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.","[15, 119]","[27, 131]"
data,Data Control,Data Control & Systems was a company formed by Rob Nursten in Zimbabwe in 1994 and became commercially operational in 1995. The company was originally a subsidiary of UUNet Internet Africa which he started in South Africa with the demand for internet services.,[0],[12]
data,Data Domain,"In data management and database analysis, a data domain is the collection of values that a data element may contain. The rule for determining the domain boundary may be as simple as a data type with an enumerated list of values.",[44],[55]
data,Data Administration,"Data administration or data resource management is an organizational function working in the areas of information systems and computer science that plans, organizes, describes and controls data resources. Data resources are usually stored in databases under a database management system or other software such as electronic spreadsheets. In many smaller organizations, data administration is performed occasionally, or is a small component of the database administrator’s work.","[0, 369]","[19, 388]"
data,Minimum Data Set,The Minimum Data Set (MDS) is part of the U.S. federally mandated process for clinical assessment of all residents in Medicare or Medicaid certified nursing homes and non-critical access hospitals with Medicare swing bed agreements. This process provides a comprehensive assessment of each resident's functional capabilities and helps nursing home and SNF staff identify health problems.,[4],[20]
data,Clinical Data Management,"Clinical data management (CDM) is a critical process in clinical research, which leads to generation of high-quality, reliable, and statistically sound data from clinical trials. Clinical data management ensures collection, integration and availability of data at appropriate quality and cost. It also supports the conduct, management and analysis of studies across the spectrum of clinical research as defined by the National Institutes of Health (NIH). The ultimate goal of CDM is to ensure that conclusions drawn from research are well supported by the data. Achieving this goal protects public health and confidence in marketed therapeutics.","[0, 179]","[24, 203]"
data,Data Wrangling,"Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one ""raw"" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.","[0, 275, 403]","[14, 289, 417]"
data,Data Reduction,"Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The purpose of data reduction can be two-fold: reduce the number of data records by eliminating invalid data or produce summary data and statistics at different aggregation levels for various applications.","[0, 187]","[14, 201]"
data,Data Classification,"In the field of data management, data classification as a part of the Information Lifecycle Management (ILM) process can be defined as a tool for categorization of data to enable/help organizations to effectively answer the following questions:What data types are available?
Where are certain data located?
What access levels are implemented?
What protection level is implemented and does it adhere to compliance regulations?",[33],[52]
data,Data Manipulation Language,"A data manipulation language (DML) is a computer programming language used for adding (inserting), deleting, and modifying (updating) data in a database. A DML is often a sublanguage of a broader database language such as SQL, with the DML comprising some of the operators in the language. Read-only selecting of data is sometimes distinguished as being part of a separate data query language (DQL), but it is closely related and sometimes also considered a component of a DML; some operators may perform both selecting (reading) and writing.",[2],[28]
data,Data Retention,"Data retention defines the policies of persistent data and records management for meeting legal and business data archival requirements. Although sometimes interchangeable, it is not to be confused with the Data Protection Act 1998.",[0],[14]
data,Data Auditing,"Data auditing is the process of conducting a data audit to assess how company's data is fit for given purpose. This involves profiling the data and assessing the impact of poor quality data on the organization's performance and profits. It can include the determination of the clarity of the data sources and can be applied in the way banks and rating agencies perform due diligence with regard to the treatment of raw data given by firms, particularly the identification of faulty data.",[0],[13]
data,Database Testing,"Database testing usually consists of a layered process, including the user interface (UI) layer, the business layer, the data access layer and the database itself. The UI layer deals with the interface design of the database, while the business layer includes databases supporting business strategies.",[0],[16]
data,Survey Data Collection,"With the application of probability sampling in the 1930s, surveys became a standard tool for empirical research in social sciences, marketing, and official statistics. The methods involved in survey data collection are any of a number of ways in which data can be collected for a statistical survey. These are methods that are used to collect information from a sample of individuals in a systematic way. First there was the change from traditional paper-and-pencil interviewing (PAPI) to computer-assisted interviewing (CAI). Now, face-to-face surveys (CAPI), telephone surveys (CATI), and mail surveys are increasingly replaced by web surveys.",[193],[215]
data,Master Data Management,"Master data management (""MDM"") is a technology-enabled discipline in which business and Information Technology (""IT"") work together to ensure the uniformity, accuracy, stewardship, semantic consistency and accountability of the enterprise's official shared master data assets.",[0],[22]
data,Open Data Protocol,"In computing, Open Data Protocol (OData) is an open protocol that allows the creation and consumption of queryable and interoperable REST APIs in a simple and standard way. Microsoft initiated OData in 2007. Versions 1.0, 2.0, and 3.0 are released under the Microsoft Open Specification Promise. Version 4.0 was standardized at OASIS, with a release in March 2014. In April 2015 OASIS submitted OData v4 and OData JSON Format v4 to ISO/IEC JTC 1 for approval as an international standard.",[14],[32]
data,Graph Database,"In computing, a graph database (GDB) is a database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data. A key concept of the system is the graph. The graph relates the data items in the store to a collection of nodes and edges, the edges representing the relationships between the nodes. The relationships allow data in the store to be linked together directly and, in many cases, retrieved with one operation. Graph databases hold the relationships between data as a priority. Querying relationships is fast because they are perpetually stored in the database. Relationships can be intuitively visualized using graph databases, making them useful for heavily inter-connected data.","[16, 469, 670]","[30, 483, 684]"
data,Data Consistency,Data consistency refers to when same data kept at different places do not match.,[0],[16]
data,IBM WebSphere DataPower SOA Appliances,"IBM WebSphere DataPower SOA Appliances is a family of pre-built, pre-configured rack mountable network devices that can help accelerate XML and Web Services deployments while extending SOA infrastructure. Originally these devices were created by DataPower Technology Inc., which was acquired by IBM in October 2005.",[0],[38]
data,Exploratory Data Analysis,"In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.","[15, 344]","[40, 369]"
data,Machine-Generated Data,"Machine-generated data is information automatically generated by a computer process, application, or other mechanism without the active intervention of a human. While the term dates back over fifty years, there is some current indecision as to the scope of the term. Monash Research's Curt Monash defines it as ""data that was produced entirely by machines OR data that is more about observing humans than recording their choices."" Meanwhile, Daniel Abadi, CS Professor at Yale, proposes a narrower definition, ""Machine-generated data is data that is generated as a result of a decision of an independent computational agent or a measurement of an event that is not caused by a human action."" Regardless of definition differences, both exclude data manually entered by a person. Machine-generated data crosses all industry sectors. Often and increasingly, humans are unaware their actions are generating the data.","[0, 511, 778]","[22, 533, 800]"
data,Data-Flow Analysis,Data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a computer program. A program's control flow graph (CFG) is used to determine those parts of a program to which a particular value assigned to a variable might propagate. The information gathered is often used by compilers when optimizing a program. A canonical example of a data-flow analysis is reaching definitions.,"[0, 400]","[18, 418]"
data,Oracle Data Guard,The software which Oracle Corporation markets as Oracle Data Guard forms an extension to the Oracle relational database management system (RDBMS). It aids in establishing and maintaining secondary standby databases as alternative/supplementary repositories to production primary databases.,[49],[66]
data,Azure Data Lake,"Azure Data Lake is a scalable data storage and analytics service. The service is hosted in Azure, Microsoft's public cloud.",[0],[15]
data,Program Database,"Program database (PDB) is a proprietary file format for storing debugging information about a program. PDB files commonly have a .pdb extension. A PDB file is typically created from source files during compilation. It stores a list of all symbols in a module with their addresses and possibly the name of the file and the line on which the symbol was declared. This symbol information is not stored in the module itself, because it takes up a lot of space.",[0],[16]
data,Data Synchronization,"Data synchronization is the process of establishing consistency among data from a source to a target data storage and vice versa and the continuous harmonization of the data over time. It is fundamental to a wide variety of applications, including file synchronization and mobile device synchronization e.g., for PDAs.

Synchronization can also be useful in encryption for synchronizing Public Key Servers.",[0],[20]
data,Data Virtualization,"Data virtualization is an approach to data management that allows an application to retrieve and manipulate data without requiring technical details about the data, such as how it is formatted at source, or where it is physically located, and can provide a single customer view of the overall data.",[0],[19]
data,Dynamic Data,"In computing, Dynamic Data Exchange (DDE) is a technology for interprocess communication used in early versions of Microsoft Windows and OS/2. DDE allows programs to manipulate objects provided by other programs, and respond to user actions affecting those objects. DDE was partially superseded by Object Linking and Embedding (OLE), and is currently maintained in Windows systems only for the sake of backward compatibility.",[14],[26]
data,Data Hub,"A data hub is a collection of data from multiple sources organized for distribution, sharing, and often subsetting and sharing. Generally this data distribution is in the form of a hub and spoke architecture.",[2],[10]
data,Data Element,"In metadata, the term data element is an atomic unit of data that has precise meaning or precise semantics. A data element has:An identification such as a data element name
A clear data element definition
One or more representation terms
Optional enumerated values Code (metadata)
A list of synonyms to data elements in other metadata registries Synonym ring","[22, 110, 155, 181, 303]","[34, 122, 167, 193, 315]"
data,Panel Data,"In statistics and econometrics, panel data and longitudinal data are both multi-dimensional data involving measurements over time. Panel data is a subset of longitudinal data where observations are for the same subjects each time.","[32, 131]","[42, 141]"
data,Data System,"Data system is a term used to refer to an organized collection of symbols and processes that may be used to operate on such symbols. Any organised collection of symbols and symbol-manipulating operations can be considered a data system. Hence, human-speech analysed at the level of phonemes can be considered a data system as can the Incan artefact of the khipu and an image stored as pixels. A data system is defined in terms of some data model and bears a resemblance to the idea of a physical symbol system.","[0, 224, 311, 395]","[11, 235, 322, 406]"
data,Cloud Database,"A cloud database is a database that typically runs on a cloud computing platform and access to the database is provided as-a-service. There are two common deployment models: users can run databases on the cloud independently, using a virtual machine image, or they can purchase access to a database service, maintained by a cloud database provider. Of the databases available on the cloud, some are SQL-based and some use a NoSQL data model.","[2, 324]","[16, 338]"
data,Data Curation,"Data curation is the organization and integration of data collected from various sources. It involves annotation, publication and presentation of the data such that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes ""all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data"". In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database.","[0, 271, 456]","[13, 284, 469]"
data,Data Masking,Data masking or data obfuscation is the process of hiding original data with modified content,[0],[12]
data,Data Management Plan,"A data management plan or DMP is a formal document that outlines how data are to be handled both during a research project, and after the project is completed. The goal of a data management plan is to consider the many aspects of data management, metadata generation, data preservation, and analysis before the project begins; this may lead to data being well-managed in the present, and prepared for preservation in the future.","[2, 174]","[22, 194]"
data,CAD Data Exchange,CAD data exchange is a modality of data exchange used to translate data between different Computer-aided design (CAD) authoring systems or between CAD and other downstream CAx systems.,[0],[17]
data,Evolution-Data Optimized,"Evolution-Data Optimized is a telecommunications standard for the wireless transmission of data through radio signals, typically for broadband Internet access. EV-DO is an evolution of the CDMA2000 (IS-2000) standard which supports high data rates and can be deployed alongside a wireless carrier's voice services. It uses advanced multiplexing techniques including code division multiple access (CDMA) as well as time division multiplexing (TDM) to maximize throughput. It is a part of the CDMA2000 family of standards and has been adopted by many mobile phone service providers around the world particularly those previously employing CDMA networks. It is also used on the Globalstar satellite phone network.",[0],[24]
data,Data Terminal Equipment,Data terminal equipment (DTE) is an end instrument that converts user information into signals or reconverts received signals. These can also be called tail circuits. A DTE device communicates with the data circuit-terminating equipment (DCE). The DTE/DCE classification was introduced by IBM.,[0],[23]
data,Database Cursor,"In computer science, a database cursor is a mechanism that enables traversal over the records in a database. Cursors facilitate subsequent processing in conjunction with the traversal, such as retrieval, addition and removal of database records. The database cursor characteristic of traversal makes cursors akin to the programming language concept of iterator.","[23, 250]","[38, 265]"
data,Data Transformation Services,"Data Transformation Services, or DTS, is a set of objects and utilities to allow the automation of extract, transform and load operations to or from a database. The objects are DTS packages and their components, and the utilities are called DTS tools. DTS was included with earlier versions of Microsoft SQL Server, and was almost always used with SQL Server databases, although it could be used independently with other databases.",[0],[28]
data,Persistent Data Structure,"In computing, a persistent data structure is a data structure that always preserves the previous version of itself when it is modified. Such data structures are effectively immutable, as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. The term was introduced in Driscoll, Sarnak, Sleator, and Tarjans' 1986 article.",[16],[41]
data,Data Literacy,"Data literacy is the ability to read, understand, create, and communicate data as information. Much like literacy as a general concept, data literacy focuses on the competencies involved in working with data. It is, however, not similar to the ability to read text since it requires certain skills involving reading and understanding data.","[0, 136]","[13, 149]"
data,Data Compression,"In signal processing, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder.","[22, 480]","[38, 496]"
data,Change Data Capture (CDC),"In databases, change data capture (CDC) is a set of software design patterns used to determine and track the data that has changed so that action can be taken using the changed data.",[14],[39]
data,Tabular Data,"Tabular Data Stream (TDS) is an application layer protocol used to transfer data between a database server and a client. It was initially designed and developed by Sybase Inc. for their Sybase SQL Server relational database engine in 1984, and later by Microsoft in Microsoft SQL Server.",[0],[12]
data,Datastax,"DataStax, Inc. is a data management company based in Santa Clara, California. Its product provides commercial support, software, and cloud database-as-a-service based on Apache Cassandra. DataStax also provides event streaming support and a cloud service based on Apache Pulsar. As of January 2021, the company has roughly 500 customers distributed in over 50 countries.","[0, 188]","[8, 196]"
data,Database Connection,"A Database connection is a facility in computer science that allows client software to talk to database server software, whether on the same machine or not. A connection is required to send commands and receive answers, usually in the form of a result set.",[2],[21]
data,Data Libraries,"A data library, data archive, or data repository is a collection of numeric and/or geospatial data sets for secondary use in research. A data library is normally part of a larger institution established for research data archiving and to serve the data users of that organisation. The data library tends to house local data collections and provides access to them through various means. A data library may also maintain subscriptions to licensed data resources for its users to access the information. Whether a data library is also considered a data archive may depend on the extent of unique holdings in the collection, whether long-term preservation services are offered, and whether it serves a broader community. Most public data libraries are listed in the Registry of Research Data Repositories.",[730],[744]
data,Tactical Data Link,"A tactical data link (TDL) uses a data link standard in order to provide communication via radio waves or cable used by NATO nations. All military C3 systems use standardized TDL to transmit, relay and receive tactical data.",[2],[20]
data,Bibliographic Databases,"A bibliographic database is a database of bibliographic records, an organized digital collection of references to published literature, including journal and newspaper articles, conference proceedings, reports, government and legal publications, patents, books, etc. In contrast to library catalogue entries, a large proportion of the bibliographic records in bibliographic databases describe articles, conference papers, etc., rather than complete monographs, and they generally contain very rich subject descriptions in the form of keywords, subject classification terms, or abstracts.",[360],[383]
data,Semi-Structured Data,"Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure.",[0],[20]
data,Data Room,"Data rooms are spaces used for housing data, usually of a secure or privileged nature. They can be physical data rooms, virtual data rooms, or data centers. They are used for a variety of purposes, including data storage, document exchange, file sharing, financial transactions, legal transactions, and more.","[0, 108, 128]","[9, 117, 137]"
data,Physician Data Query,"Physician Data Query (PDQ) is the US National Cancer Institute's (NCI) comprehensive cancer database. It contains peer-reviewed summaries on cancer treatment, screening, prevention, genetics, and supportive care, and complementary and alternative medicine; a registry of more than 6,000 open and 17,000 closed cancer clinical trials from around the world; and a directory of professionals who provide genetics services.",[0],[20]
data,Seismic Data,"Seismic data acquisition is the first of the three distinct stages of seismic exploration, the other two being seismic data processing and seismic interpretation. Seismic acquisition requires the use of a seismic source at specified locations for a seismic survey, and the energy that travels within the subsurface as seismic waves generated by the source gets recorded at specified locations on the surface by what is known as receivers.","[0, 111]","[12, 123]"
data,Database Normalization,"Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as part of his relational model.",[0],[22]
data,Data Corruption,"Data corruption refers to errors in computer data that occur during writing, reading, storage, transmission, or processing, which introduce unintended changes to the original data. Computer, transmission, and storage systems use a number of measures to provide end-to-end data integrity, or lack of errors.",[0],[15]
data,Software-Defined Data Center,"Software-defined data center is a marketing term that extends virtualization concepts such as abstraction, pooling, and automation to all data center resources and services to achieve IT as a service (ITaaS).
In a software-defined data center, ""all elements of the infrastructure — networking, storage, CPU and security – are virtualized and delivered as a service.""","[0, 214]","[28, 242]"
data,Datacap,"Datacap, a privately owned company, manufactures and sells computer software, and services. Datacap's first product, Paper Keyboard, was a ""forms processing"" product and shipped in 1989. In August 2010, IBM announced that it had acquired Datacap for an undisclosed amount.","[0, 92, 238]","[7, 99, 245]"
data,XML Databases,"An XML database is a data persistence software system that allows data to be specified, and sometimes stored, in XML format. This data can be queried, transformed, exported and returned to a calling system. XML databases are a flavor of document-oriented databases which are in turn a category of NoSQL database.",[207],[220]
data,Database Search Engine,A database search engine is a search engine that operates on material stored in a digital database.,[2],[24]
data,Data Binding,"In computer programming, data binding is a general technique that binds data sources from the provider and consumer together and synchronizes them. This is usually done with two data/information sources with different languages as in XML data binding and UI data binding. In UI data binding, data and information objects of the same language but different logic function are bound together.","[25, 238, 258, 278]","[37, 250, 270, 290]"
data,Event Data Recorder,"An event data recorder (EDR), similar to an accident data recorder (ADR) sometimes referred to informally as an automotive ""black box"", is a device installed in some automobiles to record information related to vehicle crashes or accidents. In the USA EDRs must meet federal standards, as described within the U.S. Code of Federal Regulations.",[3],[22]
data,Educational Data Mining,"Educational data mining (EDM) describes a research field concerned with the application of data mining, machine learning and statistics to information generated from educational settings. At a high level, the field seeks to develop and improve methods for exploring this data, which often has multiple levels of meaningful hierarchy, in order to discover new insights about how people learn in the context of such settings. In doing so, EDM has contributed to theories of learning investigated by researchers in educational psychology and the learning sciences. The field is closely tied to that of learning analytics, and the two have been compared and contrasted.",[0],[23]
data,Data Blending,Data blending is a process whereby big data from multiple sources are merged into a single data warehouse or data set. It concerns not merely the merging of different file formats or disparate sources of data but also different varieties of data. Data blending allows business analysts to cope with the expansion of data which they need to make critical business decisions based on good quality business intelligence.,"[0, 247]","[13, 260]"
data,Geophysical Data,"Geophysical survey is the systematic collection of geophysical data for spatial studies. Detection and analysis of the geophysical signals forms the core of Geophysical signal processing. The magnetic and gravitational fields emanating from the Earth's interior hold essential information concerning seismic activities and the internal structure. Hence, detection and analysis of the electric and Magnetic fields is very crucial. As the Electromagnetic and gravitational waves are multi-dimensional signals, all the 1-D transformation techniques can be extended for the analysis of these signals as well. Hence this article also discusses multi-dimensional signal processing techniques.",[51],[67]
data,Spatial Databases,"A spatial database is a database that is optimized for storing and querying data that represents objects defined in a geometric space. Most spatial databases allow the representation of simple geometric objects such as points, lines and polygons. Some spatial databases handle more complex structures such as 3D objects, topological coverages, linear networks, and TINs. While typical databases have developed to manage various numeric and character types of data, such databases require additional functionality to process spatial data types efficiently, and developers have often added geometry or feature data types. The Open Geospatial Consortium (OGC) developed the Simple Features specification and sets standards for adding spatial functionality to database systems. The SQL/MM Spatial ISO/IEC standard is a part the SQL/MM multimedia standard and extends the Simple Features standard with data types that support circular interpolations.","[140, 252]","[157, 269]"
data,Database Audit,"Database auditing involves observing a database so as to be aware of the actions of database users. Database administrators and consultants often set up auditing for security purposes, for example, to ensure that those without the permission to access information do not access it.",[0],[14]
data,Meter Data Management,"Meter data management (MDM) refers to software that performs long-term data storage and management for the vast quantities of data delivered by smart metering systems. This data consists primarily of usage data and events that are imported from the head-end servers managing the data collection in advanced metering infrastructure (AMI) or automatic meter reading (AMR) systems. MDM is a component in the smart grid infrastructure promoted by utility companies. This may also incorporate meter data analytics, the analysis of data emitted by electric smart meters that record consumption of electric energy.",[0],[21]
data,Database Theory,Database theory encapsulates a broad range of topics related to the study and research of the theoretical realm of databases and database management systems.,[0],[15]
data,Database Activity Monitoring,"Database activity monitoring is a database security technology for monitoring and analyzing database activity. DAM may combine data from network-based monitoring and native audit information to provide a comprehensive picture of database activity. The data gathered by DAM is used to analyze and report on database activity, support breach investigations, and alert on anomalies. DAM is typically performed continuously and in real-time.",[0],[28]
data,Data Preprocessing,"Data preprocessing is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running any analysis. 
Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology.","[0, 526]","[18, 544]"
data,Computer Data Storage,Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.,[0],[21]
data,Data Link,"The data link layer, or layer 2, is the second layer of the seven-layer OSI model of computer networking. This layer is the protocol layer that transfers data between nodes on a network segment across the physical layer. The data link layer provides the functional and procedural means to transfer data between network entities and might provide the means to detect and possibly correct errors that may occur in the physical layer.","[4, 225]","[13, 234]"
data,Remote Data Entry,A remote data entry (RDE) system is a computerized system designed for the collection of data in electronic format. The term is most commonly applied to a class of software used in the life sciences industry for collecting patient data from participants in clinical research studies—research of new drugs and/or medical devices.,[2],[19]
data,Database Encryption,"Database encryption can generally be defined as a process that uses an algorithm to transform data stored in a database into ""cipher text"" that is incomprehensible without first being decrypted. It can therefore be said that the purpose of database encryption is to protect the data stored in a database from being accessed by individuals with potentially ""malicious"" intentions. The act of encrypting a database also reduces the incentive for individuals to hack the aforementioned database as ""meaningless"" encrypted data is of little to no use for hackers. There are multiple techniques and technologies available for database encryption, the most important of which will be detailed in this article.","[0, 240, 621]","[19, 259, 640]"
data,Data Grid,"A data grid is an architecture or set of services that gives individuals or groups of users the ability to access, modify and transfer extremely large amounts of geographically distributed data for research purposes. Data grids make this possible through a host of middleware applications and services that pull together data and resources from multiple administrative domains and then present it to users upon request. The data in a data grid can be located at a single site or multiple sites where each site can be its own administrative domain governed by a set of security restrictions as to who may access the data. Likewise, multiple replicas of the data may be distributed throughout the grid outside their original administrative domain and the security restrictions placed on the original data for who may access it must be equally applied to the replicas. Specifically developed data grid middleware is what handles the integration between users and the data they request by controlling access while making it available as efficiently as possible. The adjacent diagram depicts a high level view of a data grid.","[2, 217, 434, 889, 1110]","[11, 226, 443, 898, 1119]"
data,Java Data Objects,"Java Data Objects (JDO) is a specification of Java object persistence. One of its features is a transparency of the persistence services to the domain model. JDO persistent objects are ordinary Java programming language classes (POJOs); there is no requirement for them to implement certain interfaces or extend from special classes. JDO 1.0 was developed under the Java Community Process as JSR 12. JDO 2.0 was developed under JSR 243 and was released on May 10, 2006. JDO 2.1 was completed in Feb 2008, developed by the Apache JDO project. JDO 2.2 was released in October 2008. JDO 3.0 was released in April 2010.",[0],[17]
data,Clinical Data Warehouse,"A Clinical Data Repository (CDR) or Clinical Data Warehouse (CDW) is a real time database that consolidates data from a variety of clinical sources to present a unified view of a single patient. It is optimized to allow clinicians to retrieve data for a single patient rather than to identify a population of patients with common characteristics or to facilitate the management of a specific clinical department. Typical data types which are often found within a CDR include: clinical laboratory test results, patient demographics, pharmacy information, radiology reports and images, pathology reports, hospital admission, discharge and transfer dates, ICD-9 codes, discharge summaries, and progress notes.",[36],[59]
data,Data Reference Model,The Data Reference Model (DRM) is one of the five reference models of the Federal Enterprise Architecture.,[4],[24]
data,Network Data Management Protocol,"NDMP, or Network Data Management Protocol, is a protocol meant to transport data between network attached storage (NAS) devices and backup devices. This removes the need for transporting the data through the backup server itself, thus enhancing speed and removing load from the backup server. It was originally invented by NetApp and Intelliguard, acquired by Legato and then EMC Corporation. Currently, the Storage Networking Industry Association (SNIA) oversees the development of the protocol.",[9],[41]
data,Database Partitioning,"A partition is a division of a logical database or its constituent elements into distinct independent parts. Database partitioning is normally done for manageability, performance or availability reasons, or for load balancing. It is popular in distributed database management systems, where each partition may be spread over multiple nodes, with users at the node performing local transactions on the partition. This increases performance for sites that have regular transactions involving certain views of data, whilst maintaining availability and security.",[109],[130]
data,Data Item Descriptions,"
A United States data item description (DID) is a completed document defining the data deliverables required of a United States Department of Defense contractor. A DID specifically defines the data content, format, and intended use of the data with a primary objective of achieving standardization objectives by the U.S. Department of Defense. The content and format requirements for DIDs are defined within MIL-STD-963C, Data Item Descriptions (2014).",[422],[444]
data,In-Memory Database,"An in-memory database is a database management system that primarily relies on main memory for computer data storage. It is contrasted with database management systems that employ a disk storage mechanism. In-memory databases are faster than disk-optimized databases because disk access is slower than memory access, the internal optimization algorithms are simpler and execute fewer CPU instructions. Accessing data in memory eliminates seek time when querying the data, which provides faster and more predictable performance than disk.","[3, 206]","[21, 224]"
data,Data Conditioning,"Data conditioning is the use of data management and optimization techniques which result in the intelligent routing, optimization and protection of data for storage or data movement in a computer system. Data conditioning features enable enterprise and cloud data centers to dramatically improve system utilization and increase application performance lowering both capital expenditures and operating costs.","[0, 204]","[17, 221]"
data,Data Structure Alignment,"

Data structure alignment is the way data is arranged and accessed in computer memory. It consists of three separate but related issues: data alignment, data structure padding, and packing.",[2],[26]
data,Data Remanence,"Data remanence is the residual representation of digital data that remains even after attempts have been made to remove or erase the data. This residue may result from data being left intact by a nominal file deletion operation, by reformatting of storage media that does not remove data previously written to the media, or through physical properties of the storage media that allow previously written data to be recovered. Data remanence may make inadvertent disclosure of sensitive information possible should the storage media be released into an uncontrolled environment.","[0, 425]","[14, 439]"
data,Data Cube,"In computer programming contexts, a data cube is a multi-dimensional (""n-D"") array of values. Typically, the term datacube is applied in contexts where these arrays are massively larger than the hosting computer's main memory; examples include multi-terabyte/petabyte data warehouses and time series of image data.",[36],[45]
data,Fiber Distributed Data Interface,"Fiber Distributed Data Interface (FDDI) is a standard for data transmission in a local area network.
It uses optical fiber as its standard underlying physical medium, although it was also later specified to use copper cable, in which case it may be called CDDI, standardized as TP-PMD, also referred to as TP-DDI.",[0],[32]
data,Enhanced Data Rates For GSM Evolution,Enhanced Data rates for GSM Evolution (EDGE) is a digital mobile phone technology that allows improved data transmission rates as a backward-compatible extension of GSM. EDGE is considered a pre-3G radio technology and is part of ITU's 3G definition. EDGE was deployed on GSM networks beginning in 2003 – initially by Cingular in the United States.,[0],[37]
data,Geospatial Data Abstraction Library (GDAL),"The Geospatial Data Abstraction Library (GDAL) is a computer software library for reading and writing raster and vector geospatial data formats, and is released under the permissive X/MIT style free software license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It may also be built with a variety of useful command line interface utilities for data translation and processing. Projections and transformations are supported by the PROJ library.",[4],[46]
data,Database Virtualization,"Database virtualization is the decoupling of the database layer, which lies between the storage and application layers within the application stack. Virtualization of the database layer enables a shift away from the physical, toward the logical or virtual. Virtualization enables compute and storage resources to be pooled and allocated on demand. This enables both the sharing of single server resources for multi-tenancy, as well as the pooling of server resources into a single logical database or cluster. In both cases, database virtualization provides increased flexibility, more granular and efficient allocation of pooled resources, and more scalable computing.","[0, 525]","[23, 548]"
data,Combined Information Data Network Exchange,"Combined Information Data Network Exchange, or CIDNE, is a computer system used by the US military. It is used to collect tactical information from troops.",[0],[42]
data,Data Monetization,"Data monetization, a form of monetization, may refer to the act of generating measurable economic benefits from available data sources (analytics). Less commonly, it may also refer to the act of monetizing data services. In the case of analytics, typically, these benefits accrue as revenue or expense savings, but may also include market share or corporate market value gains. Data monetization leverages data generated through business operations, available exogenous data or content, as well as data associated with individual actors such as that collected via electronic devices and sensors participating in the internet of things. For example, the ubiquity of the internet of things is generating location data and other data from sensors and mobile devices at an ever-increasing rate. When this data is collated against traditional databases, the value and utility of both sources of data increases, leading to tremendous potential to mine data for social good, research and discovery, and achievement of business objectives. Closely associated with data monetization are the emerging data as a service models for transactions involving data by the data item.","[0, 378, 1056]","[17, 395, 1073]"
data,Good Clinical Data Management Practice,"Good clinical data management practice (GCDMP) is the current industry standards for clinical data management that consist of best business practice and acceptable regulatory standards. In all phases of clinical trials, clinical and laboratory information must be collected and converted to digital form for analysis and reporting purposes. The U.S. Food and Drug Administration and International Conference on Harmonisation of Technical Requirements for Registration of Pharmaceuticals for Human Use have provided specific regulations and guidelines surrounding this component of the drug and device development process. The effective, efficient and regulatory-compliant management of clinical trial data is an essential component of drug and device development.",[0],[38]
data,Packet Data Convergence Protocol,"Packet Data Convergence Protocol (PDCP) is specified by 3GPP in TS 25.323 for UMTS, TS 36.323 for LTE and TS 38.323 for 5G New Radio (NR). PDCP is located in the Radio Protocol Stack in the UMTS/LTE/5G Air interface on top of the RLC layer.",[0],[32]
data,Automatic Identification And Data Capture,"Automatic identification and data capture (AIDC) refers to the methods of automatically identifying objects, collecting data about them, and entering them directly into computer systems, without human involvement. Technologies typically considered as part of AIDC include QR codes, bar codes, radio frequency identification (RFID), biometrics, magnetic stripes, optical character recognition (OCR), smart cards, and voice recognition. AIDC is also commonly referred to as ""Automatic Identification"", ""Auto-ID"" and ""Automatic Data Capture"".",[0],[41]
data,Data Feed,"Data feed is a mechanism for users to receive updated data from data sources. It is commonly used by real-time applications in point-to-point settings as well as on the World Wide Web. The latter is also called web feed. News feed is a popular form of web feed. RSS feed makes dissemination of blogs easy. Product feeds play increasingly important role in e-commerce and internet marketing, as well as news distribution, financial markets, and cybersecurity. Data feeds usually require structured data that include different labelled fields, such as ""title"" or ""product"".","[0, 459]","[9, 468]"
data,Connected Data Objects,Connected Data Objects (CDO) is a free implementation of a Distributed Shared Model on top of the Eclipse Modeling Framework (EMF).,[0],[22]
data,Oracle Big Data,"The Oracle Big Data Appliance consists of hardware and software from Oracle Corporation sold as a computer appliance. It was announced in 2011, promoted for consolidating and loading unstructured data into Oracle Database software.",[4],[19]
data,Ingres Database,Ingres Database is a proprietary SQL relational database management system intended to support large commercial and government applications.,[0],[15]
data,Biological Database,"Biological databases are libraries of life sciences information, collected from scientific experiments, published literature, high-throughput experiment technology, and computational analysis. They contain information from research areas including genomics, proteomics, metabolomics, microarray gene expression, and phylogenetics. Information contained in biological databases includes gene function, structure, localization, clinical effects of mutations as well as similarities of biological sequences and structures.","[0, 356]","[19, 375]"
data,Cellular Digital Packet Data,"Cellular Digital Packet Data (CDPD) was a wide-area mobile data service which used unused bandwidth normally used by AMPS mobile phones between 800 and 900 MHz to transfer data. Speeds up to 19.2 kbit/s were possible. The service was discontinued in conjunction with the retirement of the parent AMPS service; it has been functionally replaced by faster services such as 1xRTT, EV-DO, and UMTS/HSPA.",[0],[28]
data,User Datagram Protocol,"In computer networking, the User Datagram Protocol (UDP) is one of the core members of the Internet protocol suite. With UDP, computer applications can send messages, in this case referred to as datagrams, to other hosts on an Internet Protocol (IP) network. Prior communications are not required in order to set up communication channels or data paths.",[28],[50]
data,Wcf Data Services,"WCF Data Services is a platform for what Microsoft calls Data Services. It is actually a combination of the runtime and a web service through which the services are exposed. It also includes the Data Services Toolkit which lets Astoria Data Services be created from within ASP.NET itself. The Astoria project was announced at MIX 2007, and the first developer preview was made available on April 30, 2007. The first CTP was made available as a part of the ASP.NET 3.5 Extensions Preview. The final version was released as part of Service Pack 1 of the .NET Framework 3.5 on August 11, 2008. The name change from ADO.NET Data Services to WCF data Services was announced at the 2009 PDC.","[0, 637]","[17, 654]"
data,Legal Electronic Data Exchange Standard,"The Legal Electronic Data Exchange Standard is a set of file format specifications intended to facilitate electronic data transmission in the legal industry. The phrase is abbreviated LEDES and is usually pronounced as ""leeds"". The LEDES specifications are maintained by the LEDES Oversight Committee (LOC), which was formed by the Law Firm and Law Department Services Group within PricewaterhouseCoopers. Members of the committee have included law firms, corporate legal departments, universities, and software vendors. The LOC was first informally created in 1995 to address e-billing issues and then incorporated as a California mutual-benefit nonprofit corporation in 2000.",[4],[43]
data,Hierarchical Data Format,"Hierarchical Data Format (HDF) is a set of file formats designed to store and organize large amounts of data. Originally developed at the National Center for Supercomputing Applications, it is supported by The HDF Group, a non-profit corporation whose mission is to ensure continued development of HDF5 technologies and the continued accessibility of data stored in HDF.",[0],[24]
data,Dirty Data,"Dirty data, also known as rogue data, are inaccurate, incomplete or inconsistent data, especially in a computer system or database.",[0],[10]
data,Dataspaces,"Dataspaces are an abstraction in data management that aim to overcome some of the problems encountered in data integration system. The aim is to reduce the effort required to set up a data integration system by relying on existing matching and mapping generation techniques, and to improve the system in ""pay-as-you-go"" fashion as it is used. Labor-intensive aspects of data integration are postponed until they are absolutely needed.",[0],[10]
data,Clinical Data Interchange Standards Consortium,"The Clinical Data Interchange Standards Consortium (CDISC) is a standards developing organization (SDO) dealing with medical research data linked with healthcare, to ""enable information system interoperability to improve medical research and related areas of healthcare"". The standards support medical research from protocol through analysis and reporting of results and have been shown to decrease resources needed by 60% overall and 70–90% in the start-up stages when they are implemented at the beginning of the research process.",[4],[50]
data,Data Link Layer,"The data link layer, or layer 2, is the second layer of the seven-layer OSI model of computer networking. This layer is the protocol layer that transfers data between nodes on a network segment across the physical layer. The data link layer provides the functional and procedural means to transfer data between network entities and might provide the means to detect and possibly correct errors that may occur in the physical layer.","[4, 225]","[19, 240]"
data,Data Fusion,"Data fusion is the process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source.",[0],[11]
data,Raw Data,"Raw data, also known as primary data, are data collected from a source. In the context of examinations, the raw data might be described as a raw score.","[0, 108]","[8, 116]"
data,Data Transfer Object,"In the field of programming a data transfer object (DTO) is an object that carries data between processes. The motivation for its use is that communication between processes is usually done resorting to remote interfaces, where each call is an expensive operation. Because the majority of the cost of each call is related to the round-trip time between the client and the server, one way of reducing the number of calls is to use an object that aggregates the data that would have been transferred by the several calls, but that is served by one call only.",[30],[50]
data,Remote Data Capture,"Remote data capture is the process of automatic collection of scientific data. It is widely used in clinic trials, where it is referred to as electronic data capture. In physical sciences, automatic observation hardware in the field can be linked to an observer in a laboratory through a cellphone or other communication link, for example in hydrology. RDC systems influenced the design of later electronic data capture (EDC) systems.",[0],[19]
data,Qualitative Data Analysis Software,"Computer-assisted qualitative data analysis software (CAQDAS) offers tools that assist with qualitative research such as transcription analysis, coding and text interpretation, recursive abstraction, content analysis, discourse analysis, grounded theory methodology, etc.",[18],[52]
data,Database Engine Tuning Advisor,"The database engine tuning advisor (DETA) is a computer software tool for Microsoft SQL Server that enables database tuning. It can improve performance for query by tuning the indexes, creating, modifying and deleting partition and or indexes.",[4],[34]
data,Database Publishing,"Database publishing is an area of automated media production in which specialized techniques are used to generate paginated documents from source data residing in traditional databases. Common examples are mail order catalogues, direct marketing, report generation, price lists and telephone directories. The database content can be in the form of text and pictures but can also contain metadata related to formatting and special rules that may apply to the document generation process. Database publishing can be incorporated into larger workflows as a component, where documents are created, approved, revised and released.","[0, 487]","[19, 506]"
data,Data Warehouse Appliance,"In computing, the term data warehouse appliance (DWA) was coined by Foster Hinshaw for a computer architecture for data warehouses (DW) specifically marketed for big data analysis and discovery that is simple to use and high performance for the workload. A DWA includes an integrated set of servers, storage, operating systems, and databases.",[23],[47]
data,Basis Database,Basis database or OpenText Collections Server is an Extended Relational Database Management System (RDBMS) produced by OpenText.,[0],[14]
data,Hierarchical Database Model,"A hierarchical database model is a data model in which the data are organized into a tree-like structure. The data are stored as records which are connected to one another through links. A record is a collection of fields, with each field containing only one value. The  type of a record defines which fields the record contains.",[2],[29]
data,Oracle Database Appliance,"The Oracle Database Appliance (ODA) is a database server appliance made by Oracle Corporation. It was introduced in September 2011 as the mid-market offering in Oracle's family of full-stack, integrated systems the company calls engineered systems. The ODA is a single rack-mounted device providing a highly-available two-node clustered database server.",[4],[29]
data,Toad Data Modeler,"Toad Data Modeler is a database design tool allowing users to visually create, maintain, and document new or existing database systems, and to deploy changes to data structures across different platforms. It is used to construct logical and physical data models, compare and synchronize models, generate complex SQL/DDL, create and modify scripts, and reverse and forward engineer databases and data warehouse systems. Toad's data modelling software is used for database design, maintenance and documentation.",[0],[17]
data,XML Data Binding,XML data binding refers to a means of representing information in an XML document as a business object in computer memory. This allows applications to access the data in the XML from the object rather than using the DOM or SAX to retrieve the data from a direct representation of the XML itself.,[0],[16]
data,Data-Link Switching,"Data-Link Switching (DLSw) is a tunneling protocol designed to tunnel unroutable, non-IP based protocols such as IBM Systems Network Architecture (SNA) and NBF over an IP network.",[0],[19]
data,Data Localization,"
Data localization or data residency law requires data about a nation's citizens or residents to be collected, processed, and/or stored inside the country, often before being transferred internationally. Such data is usually transferred only after meeting local privacy or data protection laws, such as giving the user notice of how the information will be used and obtaining their consent.",[1],[18]
data,Google Data,"Google Data Centers are the large data center facilities Google uses to provide their services, which combine large drives, computer nodes organized in aisles of racks, internal and external networking, environmental controls, and operations software.",[0],[11]
data,Data Control Language,"A data control language (DCL) is a syntax similar to a computer programming language used to control access to data stored in a database (Authorization). In particular, it is a component of Structured Query Language (SQL). Data Control Language is one of the logical group in SQL Commands. SQL is the standard language for relational database management systems. SQL statements are used to perform tasks such as insert data to a database, delete or update data in a database, or retrieve data from a database.","[2, 223]","[23, 244]"
data,ActiveX Data Objects,"In computing, Microsoft's ActiveX Data Objects (ADO) comprises a set of Component Object Model (COM) objects for accessing data sources. A part of MDAC, it provides a middleware layer between programming languages and OLE DB. ADO allows a developer to write programs that access data without knowing how the database is implemented; developers must be aware of the database for connection only. No knowledge of SQL is required to access a database when using ADO, although one can use ADO to execute SQL commands directly.",[26],[46]
data,Data Processing Unit,"A data processing unit (DPU) is a programmable electronic component that processes streams of data. The data is transmitted to and from the component as multiplexed packets of information. DPUs have the generality and the programmability of central processing units but are specialized to operate efficiently on networking packets, storage requests or analytics requests.",[2],[22]
data,Navigational Database,"A navigational database is a type of database in which records or objects are found primarily by following references from other objects. The term was popularized by the title of Charles Bachman's 1973 Turing Award paper, The Programmer as Navigator. This paper emphasized the fact that the new disk-based database systems allowed the programmer to choose arbitrary navigational routes following relationships from record to record, contrasting this with the constraints of earlier magnetic-tape and punched card systems where data access was strictly sequential.",[2],[23]
data,Digital Data Systems,"Digital Data Systems, or DDS, was a consultancy firm specializing in operations support systems (OSS) and business support systems (BSS) in the public switched telephone network (PSTN) and carrier marketplace within the telecommunications industry. DDS was an independent provider of OSS and BSS during a time when such systems were provided by the large telecommunications switch vendors. It completed in OSS / BSS head-to-head against AT&T, Northern Telecom, and others in providing services that sold and deployed amongst the Regional Bell Operating Companies, including: Bell Atlantic, Southwestern Bell, and Pacific Telesis. It had a hardware joint venture with MIPS Computer Systems who provided the hardware platform upon which it ran. The business was acquired by Computer Sciences Corporation (CSC) in 1992.",[0],[20]
data,Simple Object Database Access,"Simple Object Database Access, also known as S.O.D.A. or SODA, is an API for database queries.",[0],[29]
data,Data Language Interface,"Data Language Interface is the language system used to access IBM's IMS databases, and its data communication system.",[0],[23]
data,Data Center Bridging,"Data center bridging (DCB) is a set of enhancements to the Ethernet local area network communication protocol for use in data center environments, in particular for use with clustering and storage area networks.",[0],[20]
data,DataCAD,"DataCAD is a computer-aided design and drafting (CADD) software for 2D and 3D architectural design and drafting, developed and sold by DATACAD LLC.","[0, 135]","[7, 142]"
data,Java Data Mining,"Java Data Mining (JDM) is a standard Java API for developing data mining applications and tools. JDM defines an object model and Java API for data mining objects and processes. JDM enables applications to integrate data mining technology for developing predictive analytics applications and tools. The JDM 1.0 standard was developed under the Java Community Process as JSR 73. In 2006, the JDM 2.0 specification was being developed under JSR 247, but has been withdrawn in 2011 without standardization.",[0],[16]
data,DNA Databases,"A DNA database or DNA databank is a database of DNA profiles which can be used in the analysis of genetic diseases, genetic fingerprinting for criminology, or genetic genealogy. DNA databases may be public or private, the largest ones being national DNA databases.","[178, 250]","[191, 263]"
data,Microsoft Data Access Components,"Microsoft Data Access Components is a framework of interrelated Microsoft technologies that allows programmers a uniform and comprehensive way of developing applications that can access almost any data store. Its components include: ActiveX Data Objects (ADO), OLE DB, and Open Database Connectivity (ODBC). There have been several deprecated components as well, such as the Microsoft Jet Database Engine, MSDASQL, and Remote Data Services (RDS). Some components have also become obsolete, such as the former Data Access Objects API and Remote Data Objects.",[0],[32]
data,Data Erasure,"
Data erasure is a software-based method of overwriting the data that aims to completely destroy all electronic data residing on a hard disk drive or other digital media by using zeros and ones to overwrite data onto all sectors of the device. By overwriting the data on the storage device, the data is rendered unrecoverable and achieves data sanitization.",[1],[13]
data,Data Encryption Standard,"The Data Encryption Standard is a symmetric-key algorithm for the encryption of digital data. Although its short key length of 56 bits makes it too insecure for applications, it has been highly influential in the advancement of cryptography.",[4],[28]
data,Remote Data Services,"Remote Data Services is an older technology that is part of Microsoft SQL Server, and used in conjunction with ActiveX Data Objects (ADO). RDS allowed the retrieval of a set of data from a database server, which the client then altered in some way and then sent back to the server for further processing. With the popular adoption of Transact-SQL, which extends the SQL programming with constructs such as loops and conditional statements, RDS became less necessary and it was eventually deprecated in Microsoft Data Access Components version 2.7. Microsoft produced SOAP Toolkit 2.0, which allows clients to do this via an open XML-based standard.",[0],[20]
data,Service Data Objects,Service Data Objects is a technology that allows heterogeneous data to be accessed in a uniform way. The SDO specification was originally developed in 2004 as a joint collaboration between Oracle (BEA) and IBM and approved by the Java Community Process in JSR 235. Version 2.0 of the specification was introduced in November 2005 as a key part of the Service Component Architecture.,[0],[20]
data,Data Link Control,"In the OSI networking model, Data Link Control (DLC) is the service provided by the data link layer. Network interface cards have a DLC address that identifies each card; for instance, Ethernet and other types of cards have a 48-bit MAC address built into the cards' firmware when they are manufactured.",[29],[46]
data,Correlation Database,"A correlation database is a database management system (DBMS) that is data-model-independent and designed to efficiently handle unplanned, ad hoc queries in an analytical system environment.",[2],[22]
data,Datakit,"Datakit is a virtual circuit switch which was developed by Sandy Fraser at Bell Labs for both local-area and wide-area networks, and in widespread deployment by the Regional Bell Operating Companies (RBOCs).",[0],[7]
data,Protein Database,"Protein database may refer to:Any protein structure database
Any protein sequence databaseExact names""Protein"" database of the National Institute of Health
Protein Database of Bio-Synthesis, Inc.

","[0, 156]","[16, 172]"
data,Remote Database Access,"Remote database access (RDA) is a protocol standard for database access produced in 1993 by the International Organization for Standardization (ISO). Despite early efforts to develop proof of concept implementations of RDA for major commercial remote database management systems (RDBMSs), this standard has not found commercial support from database vendors. The standard has since been withdrawn, and replaced by ISO/IEC 9579:1999 - Information technology -- Remote Database Access for SQL, which has also been withdrawn, and replaced by ISO/IEC 9579:2000 Information technology -- Remote database access for SQL with security enhancement.","[0, 460, 583]","[22, 482, 605]"
data,Portable Data Terminal,"A portable data terminal, or shortly PDT, is an electronic device that is used to enter or retrieve data via wireless transmission. They have also been called enterprise digital assistants (EDA), data capture mobile devices, batch terminals or just portables.",[2],[24]
data,Data Striping,"In computer data storage, data striping is the technique of segmenting logically sequential data, such as a file, so that consecutive segments are stored on different physical storage devices.",[26],[39]
data,DataNucleus,DataNucleus is an open source project which provides software products around data management in Java. The DataNucleus project started in 2008.,"[0, 107]","[11, 118]"
data,Relational Data Mining,"Relational data mining is the data mining technique for relational
databases. Unlike traditional data mining algorithms, which look for
patterns in a single table, 
relational data mining algorithms look for patterns among multiple tables
(relational patterns). For most types of propositional
patterns, there are corresponding relational patterns. For example,
there are relational classification rules, relational regression tree, and relational association rules.","[0, 165]","[22, 187]"
data,Circuit Switched Data,"In communications, Circuit Switched Data (CSD) is the original form of data transmission developed for the time-division multiple access (TDMA)-based mobile phone systems like Global System for Mobile Communications (GSM). After 2010 many telecommunication carriers dropped support for CSD, and CSD has been superseded by GPRS and EDGE (E-GPRS).",[19],[40]
data,Data Definition Specification,"In computing, a data definition specification (DDS) is a guideline to ensure comprehensive and consistent data definition. It represents the attributes required to quantify data definition. A comprehensive data definition specification encompasses enterprise data, the hierarchy of data management, prescribed guidance enforcement and criteria to determine compliance.","[16, 206]","[45, 235]"
data,Simple Data Format,"Simple Data Format (SDF) is a platform-independent, precision-preserving binary data I/O format capable of handling large, multi-dimensional arrays. It was written in 2007 by George H. Fisher, a researcher at the Space Sciences Laboratory at UC Berkeley, and released under the GNU General Public License.",[0],[18]
data,External Data Representation,"External Data Representation (XDR) is a standard data serialization format, for uses such as computer network protocols. It allows data to be transferred between different kinds of computer systems. Converting from the local representation to XDR is called encoding. Converting from XDR to the local representation is called decoding. XDR is implemented as a software library of functions which is portable between different operating systems and is also independent of the transport layer.",[0],[28]
data,Borland Database Engine,"Borland Database Engine (BDE) is the Windows-based core database engine and connectivity software behind Borland Delphi, C++Builder, IntraBuilder, Paradox for Windows, and Visual dBASE for Windows.",[0],[23]
data,Synchronous Data Link Control,"Synchronous Data Link Control (SDLC) is a computer communications protocol. It is the layer 2 protocol for IBM's Systems Network Architecture (SNA). SDLC supports multipoint links as well as error correction. It also runs under the assumption that an SNA header is present after the SDLC header. SDLC was mainly used by IBM mainframe and midrange systems; however, implementations exist on many platforms from many vendors. The use of SDLC is becoming more and more rare, mostly replaced by IP-based protocols or being tunneled through IP. In the United States, SDLC can be found in traffic control cabinets.",[0],[29]
data,Source Data,Source data is raw data that has not been processed for meaningful use to become Information.,[0],[11]
data,Remote Data Objects,"Remote Data Objects is an obsolete data access application programming interface primarily used in Microsoft Visual Basic applications on Windows 95 and later operating systems. This includes database connection, queries, stored procedures, result manipulation, and change commits. It allowed developers to create interfaces that can directly interact with Open Database Connectivity (ODBC) data sources on remote machines, without having to deal with the comparatively complex ODBC API.",[0],[19]
data,Linked Data,"In computing, linked data is structured data which is interlinked with other data so it becomes more useful through semantic queries. It builds upon standard Web technologies such as HTTP, RDF and URIs, but rather than using them to serve web pages only for human readers, it extends them to share information in a way that can be read automatically by computers. Part of the vision of linked data is for the Internet to become a global database.","[14, 386]","[25, 397]"
data,Front Panel Data Port,The front panel data port (FPDP) is a bus that provides high speed data transfer between two or more VMEbus boards at up to 160 Mbit/s with low latency. The FPDP bus uses a 32-bit parallel synchronous bus wired with an 80-conductor ribbon cable.,[4],[25]
data,Data Buffer,"In computer science, a data buffer is a region of a physical memory storage used to temporarily store data while it is being moved from one place to another. Typically, the data is stored in a buffer as it is retrieved from an input device or just before it is sent to an output device. However, a buffer may be used when moving data between processes within a computer. This is comparable to buffers in telecommunication. Buffers can be implemented in a fixed memory location in hardware—or by using a virtual data buffer in software, pointing at a location in the physical memory. In all cases, the data stored in a data buffer are stored on a physical storage medium. A majority of buffers are implemented in software, which typically use the faster RAM to store temporary data, due to the much faster access time compared with hard disk drives. Buffers are typically used when there is a difference between the rate at which data is received and the rate at which it can be processed, or in the case that these rates are variable, for example in a printer spooler or in online video streaming. In the distributed computing environment, data buffer is often implemented in the form of burst buffer that provides distributed buffering service.","[23, 511, 618, 1140]","[34, 522, 629, 1151]"
data,Datagram Transport Layer Security,"Datagram Transport Layer Security (DTLS) is a communications protocol that provides security for datagram-based applications by allowing them to communicate in a way that is designed to prevent eavesdropping, tampering, or message forgery. The DTLS protocol is based on the stream-oriented Transport Layer Security (TLS) protocol and is intended to provide similar security guarantees. The DTLS protocol datagram preserves the semantics of the underlying transport—the application does not suffer from the delays associated with stream protocols, but because it uses UDP or SCTP, the application has to deal with packet reordering, loss of datagram and data larger than the size of a datagram network packet. Because DTLS uses UDP or SCTP rather than TCP, it avoids the ""TCP meltdown problem"", when being used to create a VPN tunnel.",[0],[33]
data,Versant Object Database,Versant Object Database (VOD) is an object database software product developed by Versant Corporation.,[0],[23]
data,XML Data Package,XML Data Package (XDP) is an XML file format created by Adobe Systems in 2003. It is intended to be an XML-based companion to PDF. It allows PDF content and/or Adobe XML Forms Architecture (XFA) resources to be packaged within an XML container.,[0],[16]
data,Variable Data Intelligent Postscript Printware,Variable Data Intelligent Postscript Printware is an open language from Xerox that enables highest-performance output of variable-data PostScript documents. It is used by the FreeFlow VI Suite (VIPP) front end.,[0],[46]
data,Integrated Data Viewer (IDV),The Integrated Data Viewer (IDV) from Unidata/UCAR is a Java based software framework for analyzing and visualizing geoscience data. The IDV release includes a software library and a reference application made from that software. It uses the VisAD library and other Java-based utility packages.,[4],[32]
data,Data Scraping,Data scraping is a technique in which a computer program extracts data from human-readable output coming from another program.,[0],[13]
data,Virtual Data Room,"A virtual data room is an online repository of information that is used for the storing and distribution of documents. In many cases, a virtual data room is used to facilitate the due diligence process during an M&A transaction, loan syndication, or private equity and venture capital transactions. This due diligence process has traditionally used a physical data room to accomplish the disclosure of documents. For reasons of cost, efficiency and security, virtual data rooms have widely replaced the more traditional physical data room.","[2, 136, 459]","[19, 153, 476]"
data,Cloud Data Management Interface,"The Cloud Data Management Interface (CDMI) is a SNIA standard that specifies a protocol for self-provisioning, administering and accessing cloud storage.",[4],[35]
data,Online Music Databases,Below is a table of online music databases that are largely free of charge. Note that many of the sites provide a specialized service or focus on a particular music genre. Some of these operate as an online music store or purchase referral service in some capacity. Among the sites that have information on the largest number of entities are those sites that focus on discographies of composing and performing artists.,[20],[42]
data,Datalog,"Datalog is a declarative logic programming language that syntactically is a subset of Prolog. It is often used as a query language for deductive databases. In recent years, Datalog has found new application in data integration, information extraction, networking, program analysis, security, cloud computing and machine learning.","[0, 173]","[7, 180]"
data,Database Dump,"A database dump contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements. A database dump is most often used for backing up a database so that its contents can be restored in the event of data loss. Corrupted databases can often be recovered by analysis of the dump. Database dumps are often published by free content projects, to allow reuse or forking, as well as local searching of the database using tools such as grep","[2, 147, 338]","[15, 160, 351]"
data,Climate Data Exchange (CDX),"The Climate Data Exchange (CDX) is a JPL software framework, built on the Apache Object Oriented Data Technology (OODT) software, for sharing climate data and models.",[4],[31]
data,Data Radio Channel,"Data Radio Channel (DARC) is a high-rate standard for encoding data in a subcarrier over FM radio broadcasts. It uses a frequency of 76kHz, the fourth harmonic of the FM radio pilot tone.",[0],[18]
data,Unstructured Supplementary Service Data,"
Unstructured Supplementary Service Data (USSD), sometimes referred to as ""quick codes"" or ""feature codes"", is a communications protocol used by GSM cellular telephones to communicate with the mobile network operator's computers. USSD can be used for WAP browsing, prepaid callback service, mobile-money services, location-based content services, menu-based information services, and as part of configuring the phone on the network.",[1],[40]
data,Data Explorers,"Data Explorers is a privately owned financial data and software company headquartered in London, UK with offices in New York, US, Edinburgh and Hong Kong. The company provides financial benchmarking information to the Securities lending Industry and short-side intelligence to the Investment Management community. The company has a global dataset covering $12 trillion of securities in the lending programs of over 20,000 institutional funds.",[0],[14]
data,Data Farming,"Data farming is the process of using designed computational experiments to “grow” data, which can then be analyzed using statistical and visualization techniques to obtain insight into complex systems. These methods can be applied to any computational model.",[0],[12]
data,Programmed Data Processor,"Programmed Data Processor (PDP), referred to by some customers, media and authors as ""Programmable Data Processor, is a term used by the Digital Equipment Corporation from 1957 to 1990 for several lines of minicomputers. The name ""PDP"" intentionally avoids the use of the term ""computer"" because, at the time of the first PDPs, computers had a reputation of being large, complicated, and expensive machines, and the venture capitalists behind Digital would not support Digital's attempting to build a ""computer""; the word ""minicomputer"" had not yet been coined. So instead, Digital used their existing line of logic modules to build a Programmed Data Processor and aimed it at a market that could not afford the larger computers.","[0, 635]","[25, 660]"
data,Market Data Definition Language,"MDDL, the Market Data Definition Language, is an XML-based messaging format for exchanging information related toFinancial Instruments
Corporate events related to the financial instruments
Market-related data",[10],[41]
data,DataFlex,"DataFlex is an object-oriented high-level programming language and a fourth generation visual tool 4GL for developing Windows, web and mobile software applications on one framework-based platform. It was introduced and developed by Data Access Corporation beginning in 1982.",[0],[8]
data,Datagram,"A datagram is a basic transfer unit associated with a packet-switched network. Datagrams are typically structured in header and payload sections. Datagrams provide a connectionless communication service across a packet-switched network. The delivery, arrival time, and order of arrival of datagrams need not be guaranteed by the network.","[2, 79, 146, 289]","[10, 87, 154, 297]"
data,Datanet,"
DataNet, or Sustainable Digital Data Preservation and Access Network Partner was a research program of the U.S. National Science Foundation Office of Cyberinfrastructure. The office announced a request for proposals with this title on September 28, 2007. The lead paragraph of its synopsis describes the program as:Science and engineering research and education are increasingly digital and increasingly data-intensive. Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams. This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.",[1],[8]
data,Binary Data,"Binary data is data whose unit can take on only two possible states, traditionally labeled as 0 and 1 in accordance with the binary numeral system and Boolean algebra.",[0],[11]
data,Dataflow Architecture,"Dataflow architecture is a computer architecture that directly contrasts the traditional von Neumann architecture or control flow architecture. Dataflow architectures do not have a program counter : the executability and execution of instructions is solely determined based on the availability of input arguments to the instructions, so that the order of instruction execution is unpredictable, i.e. behavior is nondeterministic.","[0, 144]","[21, 165]"
data,Deductive Database,"A deductive database is a database system that can make deductions based on rules and facts stored in the (deductive) database. Datalog is the language typically used to specify facts, rules and queries in deductive databases. Deductive databases have grown out of the desire to combine logic programming with relational databases to construct systems that support a powerful formalism and are still fast and able to deal with very large datasets. Deductive databases are more expressive than relational databases but less expressive than logic programming systems. 
In recent years, deductive databases such as Datalog have found new application in data integration, information extraction, networking, program analysis, security, and cloud computing.","[2, 206, 227, 448, 584]","[20, 224, 245, 466, 602]"
data,Dummy Data,"In Informatics, dummy data is benign information that does not contain any useful data, but serves to reserve space where real data is nominally present. Dummy data can be used as a placeholder for both testing and operational purposes. For testing, dummy data can also be used as stubs or pad to avoid software testing issues by ensuring that all variables and data fields are occupied. In operational use, dummy data may be transmitted for OPSEC purposes. Dummy data must be rigorously evaluated and documented to ensure that it does not cause unintended effects.","[16, 154, 250, 408, 458]","[26, 164, 260, 418, 468]"
data,Object Databases,An object database is a database management system in which information is represented in the form of objects as used in object-oriented programming. Object databases are different from relational databases which are table-oriented. Object–relational databases are a hybrid of both approaches.,[150],[166]
data,Pure Data,"Pure Data (Pd) is a visual programming language developed by Miller Puckette in the 1990s for creating interactive computer music and multimedia works. While Puckette is the main author of the program, Pd is an open-source project with a large developer base working on new extensions. It is released under a license similar to the BSD license. It runs on Linux, Mac OS X, iOS, Android and Windows. Ports exist for FreeBSD and IRIX.",[0],[9]
data,Database Abstraction Layer,"A database abstraction layer is an application programming interface which unifies the communication between a computer application and databases such as SQL Server, DB2, MySQL, PostgreSQL, Oracle or SQLite. Traditionally, all database vendors provide their own interface that is tailored to their products. It is up to the application programmer to implement code for the database interfaces that will be supported by the application. Database abstraction layers reduce the amount of work by providing a consistent API to the developer and hide the database specifics behind this interface as much as possible. There exist many abstraction layers with different interfaces in numerous programming languages. If an application has such a layer built in, it is called database-agnostic.","[2, 436]","[28, 462]"
data,Digital Data Storage,"Digital Data Storage (DDS) is a computer data storage technology that is based upon the Digital Audio Tape (DAT) format that was developed during the 1980s. DDS is primarily intended for use as off-line storage, especially for generating backup copies of working data.",[0],[20]
data,Extended Data Services,"Extended Data Services, is an American standard classified under Electronic Industries Alliance standard CEA-608-E for the delivery of any ancillary data (metadata) to be sent with an analog television program, or any other NTSC video signal.",[0],[22]
data,Core Architecture Data Model,Core architecture data model (CADM) in enterprise architecture is a logical data model of information used to describe and build architectures.,[0],[28]
data,Data Center Infrastructure Efficiency,"Data center infrastructure efficiency (DCIE), is a performance improvement metric used to calculate the energy efficiency of a data center. DCIE is the percentage value derived, by dividing information technology equipment power by total facility power.",[0],[37]
data,Data Format Description Language,"Data Format Description Language, published as an Open Grid Forum Proposed Recommendation in January 2011, is a modeling language for describing general text and binary data in a standard way. A DFDL model or schema allows any text or binary data to be read from its native format and to be presented as an instance of an information set.. The same DFDL schema also allows data to be taken from an instance of an information set and written out to its native format.",[0],[32]
data,Data Link Connection Identifier,A data link connection identifier (DLCI) is a Frame Relay 10-bit-wide link-local virtual circuit identifier used to assign frames to a specific PVC or SVC. Frame Relay networks use DLCIs to statistically multiplex frames. DLCIs are preloaded into each switch and act as road signs to the traveling frames.,[2],[33]
data,Extended Display Identification Data,Extended Display Identification Data (EDID) is a metadata format for display devices to describe their capabilities to a video source. The data format is defined by a standard published by the Video Electronics Standards Association (VESA).,[0],[36]
data,LDAP Data Interchange Format,"The LDAP Data Interchange Format (LDIF) is a standard plain text data interchange format for representing LDAP directory content and update requests. LDIF conveys directory content as a set of records, one record for each object. It also represents update requests, such as Add, Modify, Delete, and Rename, as a set of records, one record for each update request.",[4],[32]
data,Multifunction Advanced Data Link,"Multifunction Advanced Data Link (MADL) is a fast switching narrow directional communications data link between stealth aircraft. It began as a method to coordinate between F-35 aircraft, but HQ Air Combat Command wants to expand the capability to coordinate future USAF strike forces of all AF stealth aircraft, including the B-2, F-22, and unmanned systems. MADL is expected to provide needed throughput, latency, frequency-hopping and anti-jamming capability with phased Array Antenna Assemblies (AAAs) that send and receive tightly directed radio signals. MADL uses the Ku band.",[0],[32]
data,Relative Record Data Set,"A relative record data set (RRDS) is a type of data set organization used by IBM's VSAM computer data storage system. Records are accessed based on their ordinal position in the file. For example, the desired record to be accessed might be the 42nd record in the file out of 999 total.",[2],[26]
data,Standard Test Data Format,"Standard Test Data Format (STDF) is a proprietary file format for semiconductor test information originally developed by Teradyne, but it is now a de facto standard widely used throughout the semiconductor industry. It is a commonly used format produced by automatic test equipment (ATE) platforms from companies such as Cohu, Roos Instruments, Teradyne, Advantest, and others.",[0],[25]
data,Climate Data Analysis Tool (CDAT),The Climate Data Analysis Tool (CDAT) is plotting software used in atmospheric sciences and climatology.,[4],[37]
analysis,Data Analysis,"Data analysis is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.","[0, 189, 396]","[13, 202, 409]"
analysis,Financial Analysis,"Financial analysis refers to an assessment of the viability, stability, and profitability of a business, sub-business or project. 
It is performed by professionals who prepare reports using ratios and other techniques, that make use of information taken from financial statements and other reports. These reports are usually presented to top management as one of their bases in making business decisions. 
Financial analysis may determine if a business will:Continue or discontinue its main operation or part of its business;
Make or purchase certain materials in the manufacture of its product;
Acquire or rent/lease certain machineries and equipment in the production of its goods;
Issue shares or negotiate for a bank loan to increase its working capital;
Make decisions regarding investing or lending capital;
Make other decisions that allow management to make an informed selection on various alternatives in the conduct of its business.","[0, 406]","[18, 424]"
analysis,Business Analysis,"Business analysis is a research discipline of identifying business needs and determining solutions to business problems. Solutions often include a software-systems development component, but may also consist of process improvements, organizational change or strategic planning and policy development. The person who carries out this task is called a business analyst or BA.",[0],[17]
analysis,Root Cause Analysis,"In science and engineering, root cause analysis (RCA) is a method of problem solving used for identifying the root causes of faults or problems. It is widely used in IT operations, telecommunications, industrial process control, accident analysis, medicine, healthcare industry, etc.",[28],[47]
analysis,Systems Analysis,"Systems analysis is ""the process of studying a procedure or business in order to identify its goals and purposes and create systems and procedures that will achieve them in an efficient way"". Another view sees system analysis as a problem-solving technique that breaks down a system into its component pieces for the purpose of the studying how well those component parts work and interact to accomplish their purpose.",[0],[16]
analysis,Requirements Analysis,"In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.",[49],[70]
analysis,Competitive Analysis,"Competitive analysis is a method invented for analyzing online algorithms, in which the performance of an online algorithm is compared to the performance of an optimal offline algorithm that can view the sequence of requests in advance. An algorithm is competitive if its competitive ratio—the ratio between its performance and the offline algorithm's performance—is bounded. Unlike traditional worst-case analysis, where the performance of an algorithm is measured only for ""hard"" inputs, competitive analysis requires that an algorithm perform well both on hard and easy inputs, where ""hard"" and ""easy"" are defined by the performance of the optimal offline algorithm.","[0, 490]","[20, 510]"
analysis,Gap Analysis,"In management literature, gap analysis involves the comparison of actual performance with potential or desired performance. If an organization does not make the best use of current resources, or forgoes investment in capital or technology, it may produce or perform below an idealized potential. This concept is similar to an economy's production being below the production possibilities frontier.",[26],[38]
analysis,Market Analysis,"A market analysis studies the attractiveness and the dynamics of a special market within a special industry. It is part of the industry analysis and thus in turn of the global environmental analysis. Through all of these analyses, the strengths, weaknesses, opportunities and threats (SWOT) of a company can be identified. Finally, with the help of a SWOT analysis, adequate business strategies of a company will be defined. The market analysis is also known as a documented investigation of a market that is used to inform a firm's planning activities, particularly around decisions of inventory, purchase, work force expansion/contraction, facility expansion, purchases of capital equipment, promotional activities, and many other aspects of a company.","[2, 429]","[17, 444]"
analysis,Technical Analysis,"In finance, technical analysis is an analysis methodology for forecasting the direction of prices through the study of past market data, primarily price and volume. Behavioral economics and quantitative analysis use many of the same tools of technical analysis, which, being an aspect of active management, stands in contradiction to much of modern portfolio theory. The efficacy of both technical and fundamental analysis is disputed by the efficient-market hypothesis, which states that stock market prices are essentially unpredictable, and research on technical analysis has produced mixed results. As such it has been described by many academics as pseudoscience.","[12, 242, 556]","[30, 260, 574]"
analysis,Trend Analysis,"Trend analysis is the widespread practice of collecting information and attempting to spot a pattern. In some fields of study, the term ""trend analysis"" has more formally defined meanings.","[0, 137]","[14, 151]"
analysis,Quantitative Analysis,"Quantitative analysis may refer to:Quantitative research, application of mathematics and statistics in economics and marketing
Quantitative analysis (chemistry), the determination of the absolute or relative abundance of one or more substances present in a sample
Quantitative analysis of behavior, quantitative models in the experimental analysis of behavior
Mathematical psychology, an approach to psychological research using mathematical modeling of perceptual, cognitive and motor processes
Statistics, the collection, organization, analysis, interpretation and presentation of data","[0, 127, 264]","[21, 148, 285]"
analysis,Performance Analysis,"Performance analysis may refer to:Performance attribution, a technique for analysing performance of funds in finance
Profiling, the analysis of computer performance

",[0],[20]
analysis,Intelligence Analysis,"Intelligence analysis is the application of individual and collective cognitive methods to weigh data and test hypotheses within a secret socio-cultural context. The descriptions are drawn from what may only be available in the form of deliberately deceptive information; the analyst must correlate the similarities among deceptions and extract a common truth. Although its practice is found in its purest form inside national intelligence agencies, its methods are also applicable in fields such as business intelligence or competitive intelligence.",[0],[21]
analysis,Process Analysis,"Process analysis is a form of technical writing and expository writing ""designed to convey to the reader how a change takes place through a series of stages"".",[0],[16]
analysis,Business Workflow Analysis,"Business Workflow Analysis (BWA), aka Business management systems p2p, is a management tool that streamlines, automates and improves the efficiency of business procedures.",[0],[26]
analysis,Financial Statement Analysis,"Financial statement analysis is the process of reviewing and analyzing a company's financial statements to make better economic decisions to earn income in future. These statements include the income statement, balance sheet, statement of cash flows, notes to accounts and a statement of changes in equity. Financial statement analysis is a method or process involving specific techniques for evaluating risks, performance, financial health, and future prospects of an organization.","[0, 307]","[28, 335]"
analysis,Credit Analysis,"Credit analysis is the method by which one calculates the creditworthiness of a business or organization. In other words, It is the evaluation of the ability of a company to honor its financial obligations. The audited financial statements of a large company might be analyzed when it issues or has issued bonds. Or, a bank may analyze the financial statements of a small business before making or renewing a commercial loan. The term refers to either case, whether the business is large or small.",[0],[15]
analysis,Failure Mode And Effects Analysis,"Failure mode and effects analysis is the process of reviewing as many components, assemblies, and subsystems as possible to identify potential failure modes in a system and their causes and effects. For each component, the failure modes and their resulting effects on the rest of the system are recorded in a specific FMEA worksheet. There are numerous variations of such worksheets. An FMEA can be a qualitative analysis, but may be put on a quantitative basis when mathematical failure rate models are combined with a statistical failure mode ratio database. It was one of the first highly structured, systematic techniques for failure analysis. It was developed by reliability engineers in the late 1950s to study problems that might arise from malfunctions of military systems. An FMEA is often the first step of a system reliability study.",[0],[33]
analysis,Job Safety Analysis,"A job safety analysis (JSA) is a procedure which helps integrate accepted safety and health principles and practices into a particular task or job operation. In a JSA, each basic step of the job is to identify potential hazards and to recommend the safest way to do the job. Other terms used to describe this procedure are job hazard analysis (JHA) and job hazard breakdown.",[2],[21]
analysis,Applied Behavior Analysis,"Applied Behavior Analysis (ABA), also called behavioral engineering, is a scientific technique concerned with applying empirical approaches based upon the principles of respondent and operant conditioning to change behavior of social significance. It is the applied form of behavior analysis; the other two forms are radical behaviorism and the experimental analysis of behavior.",[0],[25]
analysis,Engineering Analysis,"Engineering analysis involves the application of scientific analytic principles and processes to reveal the properties and state of a system, device or mechanism under study.",[0],[20]
analysis,SQL Server Analysis Services,"Microsoft SQL Server Analysis Services, SSAS, is an online analytical processing (OLAP) and data mining tool in Microsoft SQL Server. SSAS is used as a tool by organizations to analyze and make sense of information possibly spread out across multiple databases, or in disparate tables or files. Microsoft has included a number of services in SQL Server related to business intelligence and data warehousing. These services include Integration Services, Reporting Services and Analysis Services. Analysis Services includes a group of OLAP and data mining capabilities and comes in two flavors - Multidimensional and Tabular.",[10],[38]
analysis,Portfolio Analysis,"Portfolio Analysis: Advanced topics in performance measurement, risk and attribution is an industry text written by a comprehensive selection of industry experts and edited by Timothy P. Ryan. It includes chapters from practitioners and industry authors who investigate topics under the wide umbrella of performance measurement, attribution and risk management, drawing on their own experience of the fields.",[0],[18]
analysis,Price Analysis,"In marketing, Price Analysis refers to the analysis of consumer response to theoretical prices in survey research.",[14],[28]
analysis,Qualitative Analysis,"Qualitative analysis may refer to:Qualitative research, an inquiry into the reasoning behind human behavior
Qualitative inorganic analysis, a type of chemical analysis

",[0],[20]
analysis,Job Analysis,"Job analysis is a family of procedures to identify the content of a job in terms of the activities it involves in addition to the attributes or requirements necessary to perform those activities.
Job analysis provides information to organizations that helps them determine which employees are best fit for specific jobs.","[0, 196]","[12, 208]"
analysis,Structural Analysis,"Structural analysis is the determination of the effects of loads on physical structures and their components.
Structures subject to this type of analysis include all that must withstand loads, such as buildings, bridges, aircraft and ships. Structural analysis employs the fields of applied mechanics, materials science and applied mathematics to compute a structure's deformations, internal forces, stresses, support reactions, accelerations, and stability. The results of the analysis are used to verify a structure's fitness for use, often precluding physical tests. Structural analysis is thus a key part of the engineering design of structures.","[0, 241, 570]","[19, 260, 589]"
analysis,Program Analysis,"In computer science, program analysis is the process of automatically analyzing the behavior of computer programs regarding a property such as correctness, robustness, safety and liveness. 
Program analysis focuses on two major areas: program optimization and program correctness. The first focuses on improving the program’s performance while reducing the resource usage while the latter focuses on ensuring that the program does what it is supposed to do.","[21, 190]","[37, 206]"
analysis,Object-Oriented Analysis And Design,"Object-oriented analysis and design (OOAD) is a technical approach for analyzing and designing an application, system, or business by applying object-oriented programming, as well as using visual modeling throughout the software development process to guide stakeholder communication and product quality.",[0],[35]
analysis,Failure Analysis,"Failure analysis is the process of collecting and analyzing data to determine the cause of a failure, often with the goal of determining corrective actions or liability.
According to Bloch and Geitner, machinery failures reveal a reaction chain of cause and effect… usually a deficiency commonly referred to as the symptom…”. failure analysis can save money, lives, and resources if done correctly and acted upon. It is an important discipline in many branches of manufacturing industry, such as the electronics industry, where it is a vital tool used in the development of new products and for the improvement of existing products. The failure analysis process relies on collecting failed components for subsequent examination of the cause or causes of failure using a wide array of methods, especially microscopy and spectroscopy. Nondestructive testing (NDT) methods are valuable because the failed products are unaffected by analysis, so inspection sometimes starts using these methods.","[0, 326, 637]","[16, 342, 653]"
analysis,Policy Analysis,"Policy analysis is a technique used in public administration to enable civil servants, activists, and others to examine and evaluate the available options to implement the goals of laws and elected officials. The process is also used in the administration of large organizations with complex policies. It has been defined as the process of ""determining which of various policies will achieve a given set of goals in light of the relations between the policies and the goals.""",[0],[15]
analysis,Numerical Analysis,"Numerical analysis is the study of algorithms that use numerical approximation for the problems of mathematical analysis. Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. The growth in computing power has revolutionized the use of realistic mathematical models in science and engineering, and subtle numerical analysis is required to implement these detailed models of the world. For example, ordinary differential equations appear in celestial mechanics ; numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.","[0, 122, 506]","[18, 140, 524]"
analysis,Task Analysis,"Task analysis is the analysis of how a task is accomplished, including a detailed description of both manual and mental activities, task and element durations, task frequency, task allocation, task complexity, environmental conditions, necessary clothing and equipment, and any other unique factors involved in or required for one or more people to perform a given task.",[0],[13]
analysis,Comparative Analysis,Comparative analysis is type of analysis used in various of sciences and in different modifications:,[0],[20]
analysis,Profitability Analysis,"In cost accounting, profitability analysis is an analysis of the profitability of an organisation's output. Output of an organisation can be grouped into products, customers, locations, channels and/or transactions.",[20],[42]
analysis,Malware Analysis,"Malware analysis is the study or process of determining the functionality, origin and potential impact of a given malware sample such as a virus, worm, trojan horse, rootkit, or backdoor. Malware or malicious software is any computer software intended to harm the host operating system or to steal sensitive data from users, organizations or companies. Malware may include software that gathers user information without permission.",[0],[16]
analysis,Content Analysis,"Content analysis is the study of documents and communication artifacts, which might be texts of various formats, pictures, audio or video. Social scientists use content analysis to examine patterns in communication in a replicable and systematic manner. One of the key advantages of using content analysis to analyse social phenomena is its non-invasive nature, in contrast to simulating social experiences or collecting survey answers.","[0, 161, 289]","[16, 177, 305]"
analysis,Production Flow Analysis,"In operations management and industrial engineering, production flow analysis refers to methods which share the following characteristics:Classification of machines
Technological cycles information control
Generating a binary product-machines matrix ",[53],[77]
analysis,Log Analysis,"In computer log management and intelligence, log analysis is an art and science seeking to make sense out of computer-generated records. The process of creating such records is called data logging.",[45],[57]
analysis,Tolerance Analysis,"Tolerance analysis is the general term for activities related to the study of accumulated variation in mechanical parts and assemblies. Its methods may be used on other types of systems subject to accumulated variation, such as mechanical and electrical systems. Engineers analyze tolerances for the purpose of evaluating geometric dimensioning and tolerancing (GD&T). Methods include 2D tolerance stacks, 3D Monte Carlo simulations, and datum conversions.",[0],[18]
analysis,Inventory Analysis,Inventory analysis is the process of understanding the stock/product mix combined with the knowledge of the demand for stock/product. It is the technique to determine the optimum level of inventory for a firm.,[0],[18]
analysis,Static Program Analysis,"Static program analysis is the analysis of computer software that is performed without actually executing programs, in contrast with dynamic analysis, which is analysis performed on programs while they are executing. In most cases the analysis is performed on some version of the source code, and in the other cases, some form of the object code.",[0],[23]
analysis,Travel Cost Analysis,"The travel cost method of economic valuation, travel cost analysis, or Clawson method is a revealed preference method of economic valuation used in cost–benefit analysis to calculate the value of something that cannot be obtained through market prices. The aim of the method is to calculate willingness to pay for a constant price facility. The technique was first suggested by the statistician Harold Hotelling in a 1947 letter to the director of the National Park Service for a method to measure the benefit of recreation. The method was further refined by Trice and Wood (1958) and Clawson (1959). The technique is one approach to the estimation of a shadow price.",[46],[66]
analysis,Accident Analysis,"Accident analysis is carried out in order to determine the cause or causes of an accident so as to prevent further accidents of a similar kind. It is part of accident investigation or incident investigation. These analyses may be performed by a range of experts, including forensic scientists, forensic engineers or health and safety advisers. Accident investigators, particularly those in the aircraft industry, are colloquially known as ""tin-kickers"". Health and safety and patient safety professionals prefer using the term ""incident"" in place of the term ""accident"". Its retrospective nature means that accident analysis is primarily an exercise of directed explanation; conducted using the theories or methods the analyst has to hand, which directs the way in which the events, aspects, or features of accident phenomena are highlighted and explained.","[0, 607]","[17, 624]"
analysis,Security Analysis,"Security analysis is the analysis of tradeable financial instruments called securities. It deals with finding the proper value of individual securities. These are usually classified into debt securities, equities, or some hybrid of the two. Tradeable credit derivatives are also securities. Commodities or futures contracts are not securities. They are distinguished from securities by the fact that their performance is not dependent on the management or activities of an outside or third party. Options on these contracts are however considered securities, since performance is now dependent on the activities of a third party. The definition of what is and what is not a security comes directly from the language of a United States Supreme Court decision in the case of SEC v. W. J. Howey Co.. Security analysis for the purpose to state the effective value of an enterprise is typically based on the examination of fundamental business factors such as financial statements, going concern, business strategy and forecasts.","[0, 797]","[17, 814]"
analysis,Scenario Analysis,"Scenario analysis is a process of analyzing future events by considering alternative possible outcomes. Thus, scenario analysis, which is one of the main forms of projection, does not try to show one exact picture of the future. Instead, it presents several alternative future developments. Consequently, a scope of possible future outcomes is observable. Not only are the outcomes observable, also the development paths leading to the outcomes. In contrast to prognoses, the scenario analysis is not based on extrapolation of the past or the extension of past trends. It does not rely on historical data and does not expect past observations to remain valid in the future. Instead, it tries to consider possible developments and turning points, which may only be connected to the past. In short, several scenarios are fleshed out in a scenario analysis to show possible future outcomes. Each scenario normally combines optimistic, pessimistic, and more and less probable developments. However, all aspects of scenarios should be plausible. Although highly discussed, experience has shown that around three scenarios are most appropriate for further discussion and selection. More scenarios risks making the analysis overly complicated. Scenarios are often confused with other tools and approaches to planning. The flowchart to the right provides a process for classifying a phenomenon as a scenario in the intuitive logics tradition.","[0, 110, 476, 836]","[17, 127, 493, 853]"
analysis,Transferable Skills Analysis,"Transferable skills analysis is a set of tests or logic to determine what positions a person may fill if their previous position(s) no longer exists in the local job market, or they can no longer perform their last position(s). An informal transferable skills analysis can be performed with the help of a career counselor, career portfolio or a career planning article or book. Transferable skills are determined by analyzing past accomplishments or experience. For instance, a stay-at-home parent and homemaker might find they have skills in budgeting, child development, food services, property management, and so on.","[0, 240]","[28, 268]"
analysis,Performance Systems Analysis,"Behavioral systems analysis (BSA), or performance systems analysis, applies behavior analysis and systems analysis to human performance in organizations. BSA is directly related to performance management and organizational behavior management.",[38],[66]
analysis,Spend Analysis,"Spend Analysis or Spend Analytics is the process of collecting, cleansing, classifying and analyzing expenditure data with the purpose of decreasing procurement costs, improving efficiency, and monitoring controls and compliance. It can also be leveraged in other areas of business such as inventory management, contract management, complex sourcing, supplier management, budgeting, planning, and product development.",[0],[14]
analysis,Analysis Of Variance (ANOVA),"Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures used to analyze the differences among means. ANOVA was developed by the statistician Ronald Fisher. ANOVA is based on the law of total variance, where the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether two or more population means are equal, and therefore generalizes the t-test beyond two means.",[0],[28]
analysis,Thermal Analysis,"Thermal analysis is a branch of materials science where the properties of materials are studied as they change with temperature. Several methods are commonly used – these are distinguished from one another by the property which is measured:Dielectric thermal analysis): dielectric permittivity and loss factor
Differential thermal analysis: temperature difference versus temperature or time
Differential scanning calorimetry: heat flow changes versus temperature or time
Dilatometry: volume changes with temperature change
Dynamic mechanical analysis: measures storage modulus (stiffness) and loss modulus (damping) versus temperature, time and frequency
Evolved gas analysis: analysis of gases evolved during heating of a material, usually decomposition products
Laser flash analysis: thermal diffusivity and thermal conductivity
Thermogravimetric analysis: mass change versus temperature or time
Thermomechanical analysis: dimensional changes versus temperature or time
Thermo-optical analysis: optical properties
Derivatography: A complex method in thermal analysis","[0, 251, 323, 1052]","[16, 267, 339, 1068]"
analysis,Comprehensive Capital Analysis And Review (CCAR),"Comprehensive Capital Analysis and Review (CCAR) is a United States regulatory framework introduced by the Federal Reserve to assess, regulate, and supervise large banks and financial institutions – collectively referred to in the framework as bank holding companies (BHCs).",[0],[48]
analysis,Decision Analysis,"Decision analysis (DA) is the discipline comprising the philosophy, methodology, and professional practice necessary to address important decisions in a formal manner. Decision analysis includes many procedures, methods, and tools for identifying, clearly representing, and formally assessing important aspects of a decision; for prescribing a recommended course of action by applying the maximum expected-utility axiom to a well-formed representation of the decision; and for translating the formal representation of a decision and its corresponding recommendation into insight for the decision maker, and other corporate and non-corporate stakeholders.","[0, 168]","[17, 185]"
analysis,Spatial Analysis,"Spatial analysis or spatial statistics includes any of the formal techniques which studies entities using their topological, geometric, or geographic properties. Spatial analysis includes a variety of techniques, many still in their early development, using different analytic approaches and applied in fields as diverse as astronomy, with its studies of the placement of galaxies in the cosmos, to chip fabrication engineering, with its use of ""place and route"" algorithms to build complex wiring structures. In a more restricted sense, spatial analysis is the technique applied to structures at the human scale, most notably in the analysis of geographic data.","[0, 162, 538]","[16, 178, 554]"
analysis,Sensitivity Analysis,"Sensitivity analysis is the study of how the uncertainty in the output of a mathematical model or system can be divided and allocated to different sources of uncertainty in its inputs. A related practice is uncertainty analysis, which has a greater focus on uncertainty quantification and propagation of uncertainty; ideally, uncertainty and sensitivity analysis should be run in tandem.","[0, 342]","[20, 362]"
analysis,Image Analysis,Image analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.,"[0, 148]","[14, 162]"
analysis,Factor Analysis,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus ""error"" terms.","[0, 327]","[15, 342]"
analysis,Water Chemistry Analysis,"Water chemistry analyses are carried out to identify and quantify the chemical components and properties of water samples. The type and sensitivity of the analysis depends on the purpose of the analysis and the anticipated use of the water.
Chemical water analysis is carried out on water used in industrial processes, on waste-water stream, on rivers and stream, on rainfall and on the sea. In all cases the results of the analysis provides information that can be used to make decisions or to provide re-assurance that conditions are as expected.
The analytical parameters selected are chosen to be appropriate for the decision making process or to establish acceptable normality.
Water chemistry analysis is often the groundwork of studies of water quality, pollution, hydrology and geothermal waters.
Analytical methods routinely used can detect and measure all the natural elements and their inorganic compounds and a very wide range of organic chemical species using methods such as gas chromatography and mass spectrometry. In water treatment plants producing drinking water and in some industrial processes using products with distinctive taste and odours, specialised organoleptic methods may be used to detect smells at very low concentrations.",[683],[707]
analysis,Stakeholder Analysis,"Stakeholder analysis is the process of assessing a system and potential changes to it as they relate to relevant and interested parties (stakeholders). This information is used to assess how the interests of those stakeholders should be addressed in a project plan, policy, program, or other action. Stakeholder analysis is a key part of stakeholder management. A stakeholder analysis of an issue consists of weighing and balancing all of the competing demands on a firm by each of those who have a claim on it, in order to arrive at the firm's obligation in a particular case. A stakeholder analysis does not preclude the interests of the stakeholders overriding the interests of the other stakeholders affected, but it ensures that all affected will be considered.","[0, 300, 364, 580]","[20, 320, 384, 600]"
analysis,Site Analysis,"Site analysis is a preliminary phase of architectural and urban design processes dedicated to the study of the climatic, geographical, historical, legal, and infrastructural context of a specific site.",[0],[13]
analysis,Traffic Analysis,"Traffic analysis is the process of intercepting and examining messages in order to deduce information from patterns in communication, which can be performed even when the messages are encrypted. In general, the greater the number of messages observed, or even intercepted and stored, the more can be inferred from the traffic. Traffic analysis can be performed in the context of military intelligence, counter-intelligence, or pattern-of-life analysis, and is a concern in computer security.","[0, 327]","[16, 343]"
analysis,Static Timing Analysis,Static timing analysis (STA) is a simulation method of computing the expected timing of a digital circuit without requiring a simulation of the full circuit.,[0],[22]
analysis,Training Analysis,Training Analysis is the process of identifying the gap in employee training and related training needs.,[0],[17]
analysis,Situation Analysis,"Situation analysis refers to a collection of methods that managers use to analyze an organization's internal and external environment to understand the organization's capabilities, customers, and business environment. The situation analysis consists of several methods of analysis: The 5Cs Analysis, SWOT analysis and Porter five forces analysis. A Marketing Plan is created to guide businesses on how to communicate the benefits of their products to the needs of potential customer. The situation analysis is the second step in the marketing plan and is a critical step in establishing a long term relationship with customers.","[0, 222, 488]","[18, 240, 506]"
analysis,Fault Tree Analysis,"Fault tree analysis (FTA) is a top-down, deductive failure analysis in which an undesired state of a system is analyzed using Boolean logic to combine a series of lower-level events. This analysis method is mainly used in safety engineering and reliability engineering to understand how systems can fail, to identify the best ways to reduce risk and to determine event rates of a safety accident or a particular system level (functional) failure. FTA is used in the aerospace, nuclear power, chemical and process, pharmaceutical, petrochemical and other high-hazard industries; but is also used in fields as diverse as risk factor identification relating to social service system failure. FTA is also used in software engineering for debugging purposes and is closely related to cause-elimination technique used to detect bugs.",[0],[19]
analysis,Dynamic Program Analysis,"Dynamic program analysis is the analysis of computer software that is performed by executing programs on a real or virtual processor. For dynamic program analysis to be effective, the target program must be executed with sufficient test inputs to cover almost all possible outputs. Use of software testing measures such as code coverage helps ensure that an adequate slice of the program's set of possible behaviors has been observed. Also, care must be taken to minimize the effect that instrumentation has on the execution of the target program. Dynamic analysis is in contrast to static program analysis. Unit tests, integration tests, system tests and acceptance tests use dynamic testing.","[0, 138]","[24, 162]"
analysis,Exploratory Data Analysis,"In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.","[15, 344]","[40, 369]"
analysis,Data-Flow Analysis,Data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a computer program. A program's control flow graph (CFG) is used to determine those parts of a program to which a particular value assigned to a variable might propagate. The information gathered is often used by compilers when optimizing a program. A canonical example of a data-flow analysis is reaching definitions.,"[0, 400]","[18, 418]"
analysis,SWOT Analysis,"SWOT analysis is a strategic planning technique used to help a person or organization identify strengths, weaknesses, opportunities, and threats related to business competition or project planning.",[0],[13]
analysis,Environmental Analysis,"Environmental analysis is the use of analytical chemistry and other techniques to study the environment. The purpose of this is commonly to monitor and study levels of pollutants in the atmosphere, rivers and other specific settings. Other environmental analysis techniques include biological surveys or biosurvey, soil analysis or soil test, vegetation surveys and tree identification, and remote sensing which uses satellite imagery to assess the environment on different spatial scales.","[0, 240]","[22, 262]"
analysis,Cluster Analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.",[0],[16]
analysis,Transactional Analysis,"Transactional analysis (TA) is a psychoanalytic theory and method of therapy wherein social transactions are analyzed to determine the ego state of the communicator as a basis for understanding behavior. In transactional analysis, the communicator is taught to alter the ego state as a way to solve emotional problems. The method deviates from Freudian psychoanalysis which focuses on increasing awareness of the contents of subconsciously held ideas. Eric Berne developed the concept and paradigm of transactional analysis in the late 1950s.","[0, 207, 501]","[22, 229, 523]"
analysis,Oil Analysis,"Oil analysis (OA) is the laboratory analysis of a lubricant's properties, suspended contaminants, and wear debris. OA is performed during routine predictive maintenance to provide meaningful and accurate information on lubricant and machine condition. By tracking oil analysis sample results over the life of a particular machine, trends can be established which can help eliminate costly repairs. The study of wear in machinery is called tribology. Tribologists often perform or interpret oil analysis data.","[0, 264, 490]","[12, 276, 502]"
analysis,Protocol Analysis,"Protocol analysis is a psychological research method that elicits verbal reports from research participants. Protocol analysis is used to study thinking in cognitive psychology, cognitive science, and behavior analysis. It has found further application in the design of surveys and interviews, usability testing, educational psychology and design research.","[0, 109]","[17, 126]"
analysis,Survival Analysis,"Survival analysis is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems. This topic is called reliability theory or reliability analysis in engineering, duration analysis or duration modelling in economics, and event history analysis in sociology. Survival analysis attempts to answer certain questions, such as what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival?","[0, 369]","[17, 386]"
analysis,Architecture Analysis,"The Architecture Analysis & Design Language (AADL) is an architecture description language standardized by SAE. AADL was first developed in the field of avionics, and was known formerly as the Avionics Architecture Description Language.",[4],[25]
analysis,Structured Analysis,"In software engineering, structured analysis (SA) and structured design (SD) are methods for analyzing business requirements and developing specifications for converting practices into computer programs, hardware configurations, and related manual procedures.",[25],[44]
analysis,Link Analysis,"In network theory, link analysis is a data-analysis technique used to evaluate relationships (connections) between nodes. Relationships may be identified among various types of nodes (objects), including organizations, people and transactions. Link analysis has been used for investigation of criminal activity, computer security analysis, search engine optimization, market research, medical research, and art.","[19, 244]","[32, 257]"
analysis,Sensory Analysis,"Sensory analysis is a scientific discipline that applies principles of experimental design and statistical analysis to the use of human senses for the purposes of evaluating consumer products. The discipline requires panels of human assessors, on whom the products are tested, and recording the responses made by them. By applying statistical techniques to the results it is possible to make inferences and insights about the products under test. Most large consumer goods companies have departments dedicated to sensory analysis. 
Sensory analysis can mainly be broken down into three sub-sections:Analytical testing 
Affective testing 
Perception ","[0, 513, 532]","[16, 529, 548]"
analysis,Particle Size Analysis,"Particle size analysis, particle size measurement, or simply particle sizing is the collective name of the technical procedures, or laboratory techniques which determines the size range, and/or the average, or mean size of the particles in a powder or liquid sample.",[0],[22]
analysis,Multivariate Analysis,"Multivariate analysis (MVA) is based on the principles of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. Typically, MVA is used to address the situations where multiple measurements are made on each experimental unit and the relations among these measurements and their structures are important. A modern, overlapping categorization of MVA includes:Normal and general multivariate models and distribution theory
The study and measurement of relationships
Probability computations of multidimensional regions
The exploration of data structures and patterns",[0],[21]
analysis,Organizational Analysis,"In organizational theory, organizational analysis or industrial analysis is the process of reviewing the development, work environment, personnel, and operation of a business or another type of association. This review is often performed in response to crisis, but may also be carried out as part of a demonstration project, in the process of taking a program to scale, or in the course of regular operations. Conducting a periodic detailed organizational analysis can be a useful way for management to identify problems or inefficiencies that have arisen in the organization but have yet to be addressed, and develop strategies for resolving them.","[26, 441]","[49, 464]"
analysis,Life-Cycle Cost Analysis,"Life-cycle cost analysis (LCCA) is a tool to determine the most cost-effective option among different competing alternatives to purchase, own, operate, maintain and, finally, dispose of an object or process, when each is equally appropriate to be implemented on technical grounds. For example, for a highway pavement, in addition to the initial construction cost, LCCA takes into account all the user costs,, and agency costs related to future activities, including future periodic maintenance and rehabilitation. All the costs are usually discounted and total to a present-day value known as net present value (NPV). This example can be generalized on any type of material, product, or system.",[0],[24]
analysis,Seismic Analysis,"Seismic analysis is a subset of structural analysis and is the calculation of the response of a building structure to earthquakes. It is part of the process of structural design, earthquake engineering or structural assessment and retrofit in regions where earthquakes are prevalent.",[0],[16]
analysis,Sentiment Analysis,"Sentiment analysis is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.","[0, 221]","[18, 239]"
analysis,Pareto Analysis,"Pareto analysis is a formal technique useful where many possible courses of action are competing for attention. In essence, the problem-solver estimates the benefit delivered by each action, then selects a number of the most effective actions that deliver a total benefit reasonably close to the maximal possible one.",[0],[15]
analysis,Sequence Analysis,"In bioinformatics, sequence analysis is the process of subjecting a DNA, RNA or peptide sequence to any of a wide range of analytical methods to understand its features, function, structure, or evolution. Methodologies used include sequence alignment, searches against biological databases, and others. Since the development of methods of high-throughput production of gene and protein sequences, the rate of addition of new sequences to the databases increased exponentially. Such a collection of sequences does not, by itself, increase the scientist's understanding of the biology of organisms. However, comparing these new sequences to those with known functions is a key way of understanding the biology of an organism from which the new sequence comes. Thus, sequence analysis can be used to assign function to genes and proteins by the study of the similarities between the compared sequences. Nowadays, there are many tools and techniques that provide the sequence comparisons and analyze the alignment product to understand its biology.","[19, 764]","[36, 781]"
analysis,Audience Analysis,"Audience analysis is a task that is often performed by technical writers in a project's early stages. It consists of assessing the audience to make sure the information provided to them is at the appropriate level. The audience is often referred to as the end-user, and all communications need to be targeted towards the defined audience. Defining an audience requires the consideration of many factors, such as age, culture and knowledge of the subject. After considering all the known factors, a profile of the intended audience can be created, allowing writers to write in a manner that is understood by the intended audience.",[0],[17]
analysis,Social Network Analysis,"Social network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It characterizes networked structures in terms of nodes and the ties, edges, or links that connect them. Examples of social structures commonly visualized through social network analysis include social media networks, memes spread, information circulation, friendship and acquaintance networks, business networks, knowledge networks, difficult working relationships, social networks, collaboration graphs, kinship, disease transmission, and sexual relationships. These networks are often visualized through sociograms in which nodes are represented as points and ties are represented as lines. These visualizations provide a means of qualitatively assessing networks by varying the visual representation of their nodes and edges to reflect attributes of interest.","[0, 289]","[23, 312]"
analysis,Leadership Analysis,"Leadership analysis is the art of breaking down a leader into basic psychological components for study and use by academics and practitioners. Good leadership analysis is not reductionist, but rather takes into consideration the overall person in the context of the times, society and culture from which they come. Leadership analysis is traditionally housed in political psychology departments and utilizes the tools of psychology to achieve political ends by exploiting the psyche in the case of practitioners, or to gain knowledge about the building blocks of leadership and individuals in the case of academics. The distinction between the two is not made frivolously; in fact, while academics and practitioners both engage in the overarching act of analyzing leaders, they go about it quite differently. Applied analysts make great use of the psychobiography, while academics tend to analyze transcriptions in search of traits and character clues.","[0, 148, 315]","[19, 167, 334]"
analysis,Failure Mode Effects And Criticality Analysis (FMECA),Failure mode effects and criticality analysis (FMECA) is an extension of failure mode and effects analysis (FMEA).,[0],[53]
analysis,Path Analysis,"In statistics, path analysis is used to describe the directed dependencies among a set of variables. This includes models equivalent to any form of multiple regression analysis, factor analysis, canonical correlation analysis, discriminant analysis, as well as more general families of models in the multivariate analysis of variance and covariance analyses.",[15],[28]
analysis,Crime Analysis,"Crime analysis is a law enforcement function that involves systematic analysis for identifying and analyzing patterns and trends in crime and disorder. Information on patterns can help law enforcement agencies deploy resources in a more effective manner, and assist detectives in identifying and apprehending suspects. Crime analysis also plays a role in devising solutions to crime problems, and formulating crime prevention strategies. Quantitative social science data analysis methods are part of the crime analysis process, though qualitative methods such as examining police report narratives also play a role.","[0, 319, 504]","[14, 333, 518]"
analysis,Spectral Analysis,"Spectral analysis or Spectrum analysis is analysis in terms of a spectrum of frequencies or related quantities such as energies, eigenvalues, etc. In specific areas it may refer to:Spectroscopy in chemistry and physics, a method of analyzing the properties of matter from their electromagnetic interactions
Spectral estimation, in statistics and signal processing, an algorithm that estimates the strength of different frequency components of a time-domain signal. This may also be called frequency domain analysis
Spectrum analyzer, a hardware device that measures the magnitude of an input signal versus frequency within the full frequency range of the instrument
Spectral theory, in mathematics, a theory that extends eigenvalues and eigenvectors to linear operators on Hilbert space, and more generally to the elements of a Banach algebra
In nuclear and particle physics, gamma spectroscopy, and high-energy astronomy, the analysis of the output of a pulse height analyzer for characteristic features such as spectral line, edges, and various physical processes producing continuum shapes

",[0],[17]
analysis,Suitability Analysis,"Suitability Analysis is the process and procedures used to establish the suitability of a system – that is, the ability of a system to meet the needs of a stakeholder or other user.",[0],[20]
analysis,Fundamental Analysis,"Fundamental analysis, in accounting and finance, is the analysis of a business's financial statements ; health; and competitors and markets. It also considers the overall state of the economy and factors including interest rates, production, earnings, employment, GDP, housing, manufacturing and management. There are two basic approaches that can be used: bottom up analysis and top down analysis. These terms are used to distinguish such analysis from other types of investment analysis, such as quantitative and technical.",[0],[20]
analysis,Component Analysis,Component analysis is the analysis of two or more independent variables which comprise a treatment modality. It is also known as a dismantling study.,[0],[18]
analysis,Demographic Analysis,"Demographic analysis includes the things that allow us to measure the dimensions and dynamics of populations. These methods have primarily been developed to study human populations, but are extended to a variety of areas where researchers want to know how populations of social actors can change across time through processes of birth, death, and migration. In the context of human biological populations, demographic analysis uses administrative records to develop an independent estimate of the population. Demographic analysis estimates are often considered a reliable standard for judging the accuracy of the census information gathered at any time. In the labor force, demographic analysis is used to estimate sizes and flows of populations of workers; in population ecology the focus is on the birth, death, migration and immigration of individuals in a population of living organisms, alternatively, in social human sciences could involve movement of firms and institutional forms. Demographic analysis is used in a wide variety of contexts. For example, it is often used in business plans, to describe the population connected to the geographic location of the business. Demographic analysis is usually abbreviated as DA. For the 2010 U.S. Census, The U.S. Census Bureau has expanded its DA categories. Also as part of the 2010 U.S. Census, DA now also includes comparative analysis between independent housing estimates, and census address lists at different key time points.","[0, 406, 509, 674, 989, 1179]","[20, 426, 529, 694, 1009, 1199]"
analysis,Measurement Systems Analysis,"A measurement systems analysis (MSA) is a thorough assessment of a measurement process,
and typically includes a specially designed experiment that seeks to identify the components of variation in that measurement process.",[2],[30]
analysis,Dimensional Analysis,"In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities and units of measure and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often easier within the metric or SI system than in others, due to the regular 10-base in all units. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.","[28, 423]","[48, 443]"
analysis,XML For Analysis,"XML for Analysis is an industry standard for data access in analytical systems, such as OLAP and data mining. XMLA is based on other industry standards such as XML, SOAP and HTTP. XMLA is maintained by XMLA Council with Microsoft, Hyperion and SAS being the official XMLA Council founder members.",[0],[16]
analysis,Dynamic Mechanical Analysis,"Dynamic mechanical analysis is a technique used to study and characterize materials. It is most useful for studying the viscoelastic behavior of polymers. A sinusoidal stress is applied and the strain in the material is measured, allowing one to determine the complex modulus. The temperature of the sample or the frequency of the stress are often varied, leading to variations in the complex modulus; this approach can be used to locate the glass transition temperature of the material, as well as to identify transitions corresponding to other molecular motions.",[0],[27]
analysis,Gait Analysis,"Gait analysis is the systematic study of animal locomotion, more specifically the study of human motion, using the eye and the brain of observers, augmented by instrumentation for measuring body movements, body mechanics, and the activity of the muscles. Gait analysis is used to assess and treat individuals with conditions affecting their ability to walk. It is also commonly used in sports biomechanics to help athletes run more efficiently and to identify posture-related or movement-related problems in people with injuries.","[0, 255]","[13, 268]"
analysis,Colorimetric Analysis,"Colorimetric analysis is a method of determining the concentration of a chemical element or chemical compound in a solution with the aid of a color reagent. It is applicable to both organic compounds and inorganic compounds and may be used with or without an enzymatic stage. The method is widely used in medical laboratories and for industrial purposes, e.g. the analysis of water samples in connection with industrial water treatment.",[0],[21]
analysis,Business Analysis Body Of Knowledge (BABOK),"A Guide to the Business Analysis Body of Knowledge (BABOK) is a guide about business analysis, issued by the International Institute of Business Analysis (IIBA), attempting to reflect current best practice and to provide a framework that describes the areas of knowledge, with associated activities and tasks and techniques required, from the International Institute of Business Analysis</ref>",[15],[58]
analysis,User Analysis,"In systems design, user analysis is the means by which scientists, engineers and technical writers determine the characteristics of users which will influence the development of software systems or other technological products. During the process, developers in technical fields gather information about users of their products through interviews, focus groups and other forms of qualitative research. This is typically performed by forming use cases based upon the actual work flow tasks which the users will perform while using a given piece of technology. Such analyses are vital to the composition of software documentation.",[19],[32]
analysis,Principal Component Analysis,"The principal components of a collection of points in a real p-space are a sequence of  direction vectors, where the  vector is the direction of a line that best fits the data while being orthogonal to the first  vectors. Here, a best-fitting line is defined as one that minimizes the average squared distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.",[468],[496]
analysis,Motion Analysis,"Motion analysis is used in computer vision, image processing, high-speed photography and machine vision that studies methods and applications in which two or more consecutive images from an image sequences, e.g., produced by a video camera or high-speed camera, are processed to produce information based on the apparent motion in the images. In some applications, the camera is fixed relative to the scene and objects are moving around in the scene, in some applications the scene is more or less fixed and the camera is moving, and in some cases both the camera and the scene are moving.",[0],[15]
analysis,Heuristic Analysis,"Heuristic analysis is a method employed by many computer antivirus programs designed to detect previously unknown computer viruses, as well as new variants of viruses already in the ""wild"".",[0],[18]
analysis,Modal Analysis,"Modal analysis is the study of the dynamic properties of systems in the frequency domain. Examples would include measuring the vibration of a car's body when it is attached to a shaker, or the noise pattern in a room when excited by a loudspeaker.",[0],[14]
analysis,Sieve Analysis,A sieve analysis is a practice or procedure used in civil engineering and chemical engineering to assess the particle size distribution of a granular material by allowing the material to pass through a series of sieves of progressively smaller mesh size and weighing the amount of material that is stopped by each sieve as a fraction of the whole mass.,[2],[16]
analysis,Uncertainty Analysis,"Uncertainty analysis investigates the uncertainty of variables that are used in decision-making problems in which observations and models represent the knowledge base. In other words, uncertainty analysis aims to make a technical contribution to decision-making through the quantification of uncertainties in the relevant variables.","[0, 184]","[20, 204]"
analysis,Document Layout Analysis,"In computer vision or natural language processing, document layout analysis is the process of identifying and categorizing the regions of interest in the scanned image of a text document. A reading system requires the segmentation of text zones from non-textual ones and the arrangement in their correct reading order. Detection and labeling of the different zones as text body, illustrations, math symbols, and tables embedded in a document is called geometric layout analysis. But text zones play different logical roles inside the document and this kind of semantic labeling is the scope of the logical layout analysis.",[51],[75]
analysis,Market Share Analysis,Market share analysis is a part of market analysis and indicates how well a firm is doing in the marketplace compared to its competitors.,[0],[21]
analysis,Elemental Analysis,"Elemental analysis is a process where a sample of some material is analyzed for its elemental and sometimes isotopic composition. Elemental analysis can be qualitative, and it can be quantitative. Elemental analysis falls within the ambit of analytical chemistry, the set of instruments involved in deciphering the chemical nature of our world.","[0, 130, 197]","[18, 148, 215]"
analysis,Variable Rules Analysis,"In linguistics, variable rules analysis is a set of statistical analysis methods commonly used in sociolinguistics and historical linguistics to describe patterns of variation between alternative forms in language use. It is also sometimes known as Varbrul analysis, after the name of a software package dedicated to carrying out the relevant statistical computations. The method goes back to a theoretical approach developed by the sociolinguist William Labov in the late 1960s and early 1970s, and its mathematical implementation was developed by Henrietta Cedergren and David Sankoff in 1974.",[16],[39]
analysis,Hair Analysis,"Hair analysis may refer to the chemical analysis of a hair sample, but can also refer to microscopic analysis or comparison. Chemical hair analysis may be considered for retrospective purposes when blood and urine are no longer expected to contain a particular contaminant, typically three months or less.","[0, 134]","[13, 147]"
analysis,Conjoint Analysis,Conjoint analysis is a survey-based statistical technique used in market research that helps determine how people value different attributes that make up an individual product or service.,[0],[17]
analysis,Moisture Analysis,"Moisture analysis covers a variety of methods for measuring moisture content in both high level and trace amounts in solids, liquids, or gases. Moisture in percentage amounts is monitored as a specification in commercial food production. There are many applications where trace moisture measurements are necessary for manufacturing and process quality assurance. Trace moisture in solids must be controlled for plastics, pharmaceuticals and heat treatment processes. Gas or liquid measurement applications include dry air, hydrocarbon processing, pure semiconductor gases, bulk pure gases, dielectric gases such as those in transformers and power plants, and natural gas pipeline transport.",[0],[17]
analysis,Nodal Analysis,"In electric circuits analysis, nodal analysis, node-voltage analysis, or the branch current method is a method of determining the voltage between ""nodes"" in an electrical circuit in terms of the branch currents.",[31],[45]
analysis,News Analysis,"In trading strategy, news analysis refers to the measurement of the various qualitative and quantitative attributes of textual news stories. Some of these attributes are: sentiment, relevance, and novelty. Expressing news stories as numbers and metadata permits the manipulation of everyday information in a mathematical and statistical way. This data is often used in financial markets as part of a trading strategy or by businesses to judge market sentiment and make better business decisions.",[21],[34]
analysis,Special Core Analysis,"In the petroleum industry, special core analysis, often abbreviated SCAL or SPCAN, is a laboratory procedure for conducting flow experiments on core plugs taken from a petroleum reservoir. 
Special core analysis is distinguished from ""routine (RCAL) or conventional (CCAL) core analysis"" by adding more experiments, in particular including measurements of two-phase flow properties, determining relative permeability, capillary pressure, wettability, and electrical properties.
Due to the time-consuming and costly character of SCAL measurements, routine core analysis (RCAL) data should be inspected thoroughly to select a representative subset of samples for SCAL.","[27, 190]","[48, 211]"
analysis,Concept Analysis,"Formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a subset of the objects in the concepts above it. The term was introduced by Rudolf Wille in 1981, and builds on the mathematical theory of lattices and ordered sets that was developed by Garrett Birkhoff and others in the 1930s.",[7],[23]
analysis,OLIGO Primer Analysis Software,"OLIGO Primer Analysis Software was the first publicly available software for DNA primer design. The first papers describing this software were published in 1989 and 1990, and consecutive upgrades in the 1990s and 2000s, all have been cited together over 600 times in scientific journals and over 500 times in patents. The program is a comprehensive real time PCR primer and probe search and analysis tool, and also does other tasks such as siRNA and molecular beacon searches, open reading frame and restriction enzyme analysis etc. It has been created and maintained by Wojciech Rychlik and Piotr Rychlik. The OLIGO has been reviewed several times in scientific journals, for the first time in 1991 in a review in Critical Reviews in Biochemistry and Molecular Biology, and for its next upgrades.",[0],[30]
analysis,Semen Analysis,"A semen analysis, also called seminogram, or spermiogram evaluates certain characteristics of a male's semen and the sperm contained therein. It is done to help evaluate male fertility, whether for those seeking pregnancy or verifying the success of vasectomy. Depending on the measurement method, just a few characteristics may be evaluated or many characteristics may be evaluated. Collection techniques and precise measurement method may influence results.",[2],[16]
analysis,Semantic Analysis,"In linguistics, semantic analysis is the process of relating syntactic structures, from the levels of phrases, clauses, sentences and paragraphs to the level of the writing as a whole, to their language-independent meanings. It also involves removing features specific to particular linguistic and cultural contexts, to the extent that such a project is possible. The elements of idiom and figurative speech, being cultural, are often also converted into relatively invariant meanings in semantic analysis. Semantics, although related to pragmatics, is distinct in that the former deals with word or sentence choice in any given context, while pragmatics considers the unique or particular meaning derived from context or tone. To reiterate in different terms, semantics is about universally coded meaning, and pragmatics, the meaning encoded in words that is then interpreted by an audience.","[16, 488]","[33, 505]"
analysis,Frequency Analysis,"In cryptanalysis, frequency analysis is the study of the frequency of letters or groups of letters in a ciphertext. The method is used as an aid to breaking classical ciphers.",[18],[36]
analysis,Combustion Analysis,"Combustion analysis is a method used in both organic chemistry and analytical chemistry to determine the elemental composition of a pure organic compound by combusting the sample under conditions where the resulting combustion products can be quantitatively analyzed. Once the number of moles of each combustion product has been determined the empirical formula or a partial empirical formula of 
the original compound can be calculated.",[0],[19]
analysis,Harmonic Analysis,"Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms. In the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience.",[0],[17]
analysis,Microarray Analysis,"Microarray analysis techniques are used in interpreting the data generated from experiments on DNA, RNA, and protein microarrays, which allow researchers to investigate the expression state of a large number of genes - in many cases, an organism's entire genome - in a single experiment. Such experiments can generate very large amounts of data, allowing researchers to assess the overall state of a cell or organism. Data in such large quantities is difficult - if not impossible - to analyze without the help of computer programs.",[0],[19]
analysis,Experimental Analysis Of Behavior,"The experimental analysis of behavior is school of thought in psychology founded on B. F. Skinner's philosophy of radical behaviorism and defines the basic principles used in applied behavior analysis. A central principle was the inductive reasoning data-driven examination of functional relations, as opposed to the kinds of hypothetico-deductive learning theory that had grown up in the comparative psychology of the 1920–1950 period. Skinner's approach was characterized by observation of measurable behavior which could be predicted and controlled. It owed its early success to the effectiveness of Skinner's procedures of operant conditioning, both in the laboratory and in behavior therapy.",[4],[37]
analysis,Linear Discriminant Analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.",[0],[28]
analysis,Single-Cell Analysis,"In the field of cellular biology, single-cell analysis is the study of genomics, transcriptomics, proteomics, metabolomics and cell–cell interactions at the single cell level. Due to the heterogeneity seen in both eukaryotic and prokaryotic cell populations, analyzing a single cell makes it possible to discover mechanisms not seen when studying a bulk population of cells. Technologies such as fluorescence-activated cell sorting (FACS) allow the precise isolation of selected single cells from complex samples, while high throughput single cell partitioning technologies, enable the simultaneous molecular analysis of hundreds or thousands of single unsorted cells; this is particularly useful for the analysis of transcriptome variation in genotypically identical cells, allowing the definition of otherwise undetectable cell subtypes. The development of new technologies is increasing our ability to analyze the genome and transcriptome of single cells, as well as to quantify their proteome and metabolome. Mass spectrometry techniques have become important analytical tools for proteomic and metabolomic analysis of single cells. Recent advances have enabled quantifying thousands of protein across hundreds of single cells, and thus make possible new types of analysis. In situ sequencing and fluorescence in situ hybridization (FISH) do not require that cells be isolated and are increasingly being used for analysis of tissues.",[34],[54]
analysis,Thermogravimetric Analysis,"Thermogravimetric analysis or thermal gravimetric analysis (TGA) is a method of thermal analysis in which the mass of a sample is measured over time as the temperature changes. This measurement provides information about physical phenomena, such as phase transitions, absorption, adsorption and desorption; as well as chemical phenomena including chemisorptions, thermal decomposition, and solid-gas reactions.",[0],[26]
analysis,Material Flow Analysis,"Material flow analysis (MFA), also referred to as substance flow analysis (SFA), is an analytical method to quantify flows and stocks of materials or substances in a well-defined system. MFA is an important tool to study the bio-physical aspects of human activity on different spatial and temporal scales. It is considered a core method of industrial ecology or anthropogenic, urban, social and industrial metabolism. MFA is used to study material, substance, or product flows across different industrial sectors or within ecosystems. MFA can also be applied to a single industrial installation, for example, for tracking nutrient flows through a waste water treatment plant. When combined with an assessment of the costs associated with material flows this business-oriented application of MFA is called material flow cost accounting. MFA is an important tool to study the circular economy and to devise material flow management. Since the 1990s, the number of publications related to material flow analysis has grown steadily. Peer-reviewed journals that publish MFA-related work include the Journal of Industrial Ecology, Ecological Economics, Environmental Science and Technology, and Resources, Conservation, and Recycling.","[0, 986]","[22, 1008]"
analysis,Context Analysis,"Context analysis is a method to analyze the environment in which a business operates. Environmental scanning mainly focuses on the macro environment of a business. But context analysis considers the entire environment of a business, its internal and external environment. This is an important aspect of business planning. One kind of context analysis, called SWOT analysis, allows the business to gain an insight into their strengths and weaknesses and also the opportunities and threats posed by the market within which they operate. The main goal of a context analysis, SWOT or otherwise, is to analyze the environment in order to develop a strategic plan of action for the business.","[0, 168, 334, 554]","[16, 184, 350, 570]"
analysis,Analog Signature Analysis,Analog signature analysis is electronic component and circuit board troubleshooting technique which applies a current-limited AC sinewave across two points of an electronic component or circuit.,[0],[25]
analysis,Decline Curve Analysis,"Decline curve analysis is a means of predicting future oil well or gas well production based on past production history. Production decline curve analysis is a traditional means of identifying well production problems and predicting well performance and life based on measured oil well production. 
Before the availability of computers, decline curve analysis was performed by hand on semi-log plot paper. Currently, decline curve analysis software on PC computers is used to plot production decline curves for petroleum economics analysis.","[0, 132, 337, 417]","[22, 154, 359, 439]"
analysis,Gravimetric Analysis,"Gravimetric analysis describes a set of methods used in analytical chemistry for the quantitative determination of an analyte based on its mass. The principle of this type of analysis is that once an ion's mass has been determined as a unique compound, that known measurement can then be used to determine the same analyte's mass in a mixture, as long as the relative quantities of the other constituents are known.",[0],[20]
analysis,Analysis Of Covariance,"Analysis of covariance (ANCOVA) is a general linear model which blends ANOVA and regression. ANCOVA evaluates whether the means of a dependent variable (DV) are equal across levels of a categorical independent variable (IV) often called a treatment, while statistically controlling for the effects of other continuous variables that are not of primary interest, known as covariates (CV) or nuisance variables. Mathematically, ANCOVA decomposes the variance in the DV into variance explained by the CV(s), variance explained by the categorical IV, and residual variance. Intuitively, ANCOVA can be thought of as 'adjusting' the DV by the group means of the CV(s).",[0],[22]
analysis,Matrix Analysis,"In mathematics, particularly in linear algebra and applications, matrix analysis is the study of matrices and their algebraic properties. Some particular topics out of many include; operations defined on matrices, functions of matrices, and the eigenvalues of matrices.",[65],[80]
analysis,Potential Analysis,"Potential analysis describes the structural examination of specific characteristics and competencies. Potential analyses provide information about abilities of employees, future events, methods or organizations. Due to that the analysis of the branch of production, the financial sphere, the research & development and the Human resources is differentiated.",[0],[18]
analysis,Correspondence Analysis,"Correspondence analysis (CA) or reciprocal averaging is a multivariate statistical technique proposed by Herman Otto Hartley (Hirschfeld) and later developed by Jean-Paul Benzécri. It is conceptually similar to principal component analysis, but applies to categorical rather than continuous data. In a similar manner to principal component analysis, it provides a means of displaying or summarising a set of data in two-dimensional graphical form. Its aim is to display in a biplot any structure hidden in the multivariate setting of the data table. As such it is a technique from the field of multivariate ordination. Since the variant of CA described here can by applied either with a focus on the rows or on the columns it should in fact be called simple (symmetric) correspondence analysis.","[0, 770]","[23, 793]"
analysis,Qualitative Data Analysis Software,"Computer-assisted qualitative data analysis software (CAQDAS) offers tools that assist with qualitative research such as transcription analysis, coding and text interpretation, recursive abstraction, content analysis, discourse analysis, grounded theory methodology, etc.",[18],[52]
analysis,Fourier Analysis,"In mathematics, Fourier analysis is the study of the way general functions may be represented or approximated by sums of simpler trigonometric functions. Fourier analysis grew from the study of Fourier series, and is named after Joseph Fourier, who showed that representing a function as a sum of trigonometric functions greatly simplifies the study of heat transfer.","[16, 154]","[32, 170]"
analysis,Non-Classical Analysis,"In mathematics, non-classical analysis is any system of analysis, other than classical real analysis, and complex, vector, tensor, etc., analysis based upon it.",[16],[38]
analysis,Audio Analysis,"Audio analysis refers to the extraction of information and meaning from audio signals for analysis, classification, storage, retrieval, synthesis, etc. The observation mediums and interpretation methods vary, as audio analysis can refer to the human ear and how people interpret the audible sound source, or it could refer to using technology such as an Audio analyzer to evaluate other qualities of a sound source such as amplitude, distortion, frequency response, and more. Once an audio source's information has been observed, the information revealed can then be processed for the logical, emotional, descriptive, or otherwise relevant interpretation by the user.","[0, 212]","[14, 226]"
analysis,Frame Analysis,"Frame analysis is a multi-disciplinary social science research method used to analyze how people understand situations and activities. Frame analysis looks at images, stereotypes, metaphors, actors, messages, and more. It examines how important these factors are and how and why they are chosen. The concept is generally attributed to the work of Erving Goffman and his 1974 book Frame analysis: An essay on the organization of experience and has been developed in social movement theory, policy studies and elsewhere.","[0, 135, 380]","[14, 149, 394]"
analysis,Independent Component Analysis,"In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the ""cocktail party problem"" of listening in on one person's speech in a noisy room.",[22],[52]
analysis,Discourse Analysis,"Discourse analysis (DA), or discourse studies, is an approach to the analysis of written, vocal, or sign language use, or any significant semiotic event.",[0],[18]
analysis,Event Tree Analysis,"Event tree analysis (ETA) is a forward, top-down, logical modeling technique for both success and failure that explores responses through a single initiating event and lays a path for assessing probabilities of the outcomes and overall system analysis. This analysis technique is used to analyze the effects of functioning or failed systems given that an event has occurred. ETA is a powerful tool that will identify all consequences of a system that have a probability of occurring after an initiating event that can be applied to a wide range of systems including: nuclear power plants, spacecraft, and chemical plants. This technique may be applied to a system early in the design process to identify potential issues that may arise, rather than correcting the issues after they occur. With this forward logic process, use of ETA as a tool in risk assessment can help to prevent negative outcomes from occurring, by providing a risk assessor with the probability of occurrence. ETA uses a type of modeling technique called event tree, which branches events from one single event using Boolean logic.",[0],[19]
analysis,Qualitative Inorganic Analysis,"Classical qualitative inorganic analysis is a method of analytical chemistry which seeks to find the elemental composition of inorganic compounds. It is mainly focused on detecting ions in an aqueous solution, therefore materials in other forms may need to be brought to this state before using standard methods. The solution is then treated with various reagents to test for reactions characteristic of certain ions, which may cause color change, precipitation and other visible changes.",[10],[40]
analysis,Viewshed Analysis,"Viewshed analysis is a computational algorithm that delineates a viewshed, the area that is visible from a given location. It is a common part of the terrain analysis toolset found in of most geographic information system (GIS) software. The analysis uses the elevation value of each cell of the digital elevation model (DEM) to determine visibility to or from a particular cell. The location of this particular cell varies depending on the needs of the analysis. 
For example, a viewshed analysis is commonly used to locate communication towers or determining the view from a road. Viewsheds can be calculated using an individual point such as a tower or multiple points such as a line representing a road. When analyzing a line segment, each of the vertices along the line is calculated to determine its visible area. The process can also be reversed. For example, when locating a landfill, the analysis can determine from where the landfill is visible to keep it hidden from view.","[0, 480]","[17, 497]"
analysis,Abc Analysis,"In materials management, ABC analysis is an inventory categorization technique. ABC analysis divides an inventory into three categories—""A items"" with very tight control and accurate records, ""B items"" with less tightly controlled and good records, and ""C items"" with the simplest controls possible and minimal records.","[25, 80]","[37, 92]"
analysis,Multivariate Analysis Of Variance,"In statistics, multivariate analysis of variance (MANOVA) is a procedure for comparing multivariate sample means. As a multivariate procedure, it is used when there are two or more dependent variables, and is often followed by significance tests involving individual dependent variables separately.",[15],[48]
analysis,Metabolic Flux Analysis,"Metabolic flux analysis (MFA) is an experimental fluxomics technique used to examine production and consumption rates of metabolites in a biological system. At an intracellular level, it allows for the quantification of metabolites, thereby elucidating the central metabolism of the cell. Employing stoichiometric models of metabolism and mass spectrometry methods with isotopic mass resolution, the transfer of moieties containing isotopic tracers from one metabolite into another can be elucidated, and information about the metabolic network thus derived. Metabolic flux analysis (MFA) has many applications such as determining the limits on the ability of a biological system in producing a biochemical such as ethanol and predicting the response to gene additions or knockouts.","[0, 559]","[23, 582]"
analysis,PEST Analysis,"PEST analysis describes a framework of macro-environmental factors used in the environmental scanning component of strategic management. It is part of an external analysis when conducting a strategic analysis or doing market research, and gives an overview of the different macro-environmental factors to be taken into consideration. It is a strategic tool for understanding market growth or decline, business position, potential and direction for operations.",[0],[13]
analysis,Bloodstain Pattern Analysis,"Bloodstain Pattern Analysis (BPA) is the study and analysis of bloodstains at a known or suspected crime scene with the purpose of drawing conclusions about the nature, timing and other details of the crime. The patterns of the blood stain can help in crime scene reconstruction. It is used to study homicide or other violent crimes in which a lot of blood may be present. It is one of the several specialties of forensic science. The use of bloodstains as evidence is not new. Since the late 1950s, BPA experts have claimed to be able to use biology, physics, and mathematical calculations to reconstruct with accuracy events at a crime scene, and these claims have been accepted by the criminal justice system. It uses an analysis of the color, shape, and size of blood stains at a crime scene, as well as principles of fluid mechanics and biology to draw conclusions about the nature and proceedings of a crime scene.",[0],[27]
analysis,Thermomechanical Analysis,"Thermomechanical analysis (TMA) is a technique used in thermal analysis, a branch of materials science which studies the properties of materials as they change with temperature.",[0],[25]
analysis,Satellite Analysis Branch,"The United States Satellite Analysis Branch, part of National Oceanic and Atmospheric Administration (NOAA)'s National Environmental Satellite, Data, and Information Service's Satellite Services Division, is the operational focal point for real-time imagery products within NESDIS. It is also responsible for doing Dvorak technique intensity fixes on tropical cyclones. Its roots lie in the establishment of the Meteorological Satellite Section by January 1959.",[18],[43]
analysis,Lexical Analysis,"In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters into a sequence of tokens. A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, although scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.","[21, 167]","[37, 183]"
analysis,Melting Curve Analysis,"Melting curve analysis is an assessment of the dissociation characteristics of double-stranded DNA during heating. As the temperature is raised, the double strand begins to dissociate leading to a rise in the absorbance intensity, hyperchromicity. The temperature at which 50% of DNA is denatured is known as the melting temperature.",[0],[22]
analysis,Dissolved Gas Analysis,"Dissolved gas analysis (DGA) is an examination of electrical transformer oil contaminants. Insulating materials within electrical equipment liberate gases as they slowly break down over time. The composition and distribution of these dissolved gases are indicators of the effects of deterioration, such as pyrolysis or partial discharge, and the rate of gas generation indicates the severity. DGA is beneficial to a preventive maintenance program.",[0],[22]
analysis,Flux Balance Analysis,"Flux balance analysis (FBA) is a mathematical method for simulating metabolism in genome-scale reconstructions of metabolic networks. In comparison to traditional methods of modeling, FBA is less intensive in terms of the input data required for constructing the model. Simulations performed using FBA are computationally inexpensive and can calculate steady-state metabolic fluxes for large models in a few seconds on modern personal computers.",[0],[21]
analysis,System For Electronic Document Analysis And Retrieval,"The System for Electronic Document Analysis and Retrieval (SEDAR) is a mandatory document filing and retrieval system for Canadian public companies. It is similar to EDGAR, the filing system operated by the Securities and Exchange Commission for United States public companies. SEDAR is administered by the Canadian Securities Administrators, a coordinating body comprising the 13 Canadian provincial and territorial securities commissions, and operated on their behalf since 2014 by the Alberta Securities Commission.",[4],[57]
analysis,Bioelectrical Impedance Analysis,"Bioelectrical impedance analysis (BIA) is a commonly used method for estimating body composition, in particular body fat and muscle mass. In BIA, a weak electric current flows through the body and the voltage is measured in order to calculate impedance (resistance) of the body. Most body water is stored in muscle. Therefore, if a person is more muscular there is a high chance that the person will also have more body water, which leads to lower impedance. Since the advent of the first commercially available devices in the mid-1980s the method has become popular owing to its ease of use and portability of the equipment. It is familiar in the consumer market as a simple instrument for estimating body fat. BIA actually determines the electrical impedance, or opposition to the flow of an electric current through body tissues which can then be used to estimate total body water (TBW), which can be used to estimate fat-free body mass and, by difference with body weight, body fat.",[0],[32]
analysis,Neutron Activation Analysis,"Neutron activation analysis (NAA) is the nuclear process used for determining the concentrations of elements in a vast amount of materials. NAA allows discrete sampling of elements as it disregards the chemical form of a sample, and focuses solely on its nucleus. The method is based on neutron activation and therefore requires a source of neutrons. The sample is bombarded with neutrons, causing the elements to form radioactive isotopes. The radioactive emissions and radioactive decay paths for each element are well known. Using this information, it is possible to study spectra of the emissions of the radioactive sample, and determine the concentrations of the elements within it. A particular advantage of this technique is that it does not destroy the sample, and thus has been used for analysis of works of art and historical artifacts. NAA can also be used to determine the activity of a radioactive sample.",[0],[27]
analysis,Statistical Coupling Analysis,"Statistical coupling analysis or SCA is a technique used in bioinformatics to measure covariation between pairs of amino acids in a protein multiple sequence alignment (MSA). More specifically, it quantifies how much the amino acid distribution at some position i changes upon a perturbation of the amino acid distribution at another position j. The resulting statistical coupling energy indicates the degree of evolutionary dependence between the residues, with higher coupling energy corresponding to increased dependence.",[0],[29]
analysis,Multivariate Analysis Of Covariance (MANCOVA),"Multivariate analysis of covariance (MANCOVA) is an extension of analysis of covariance (ANCOVA) methods to cover cases where there is more than one dependent variable and where the control of concomitant continuous independent variables – covariates – is required. The most prominent benefit of the MANCOVA design over the simple MANOVA is the 'factoring out' of noise or error that has been introduced by the covariant. A commonly used multivariate version of the ANOVA F-statistic is Wilks' Lambda (Λ), which represents the ratio between the error variance and the effect variance.",[0],[45]
analysis,Modeling And Analysis Of Real Time And Embedded Systems,Modeling and Analysis of Real Time and Embedded systems also known as MARTE is the OMG standard for modeling real-time and embedded applications with UML2.,[0],[55]
analysis,Structured Analysis And Design Technique,"Structured analysis and design technique (SADT) is a systems engineering and software engineering methodology for describing systems as a hierarchy of functions. SADT is a structured analysis modelling language, which uses two types of diagrams: activity models and data models. It was developed in the late 1960s by Douglas T. Ross, and was formalized and published as IDEF0 in 1981.",[0],[40]
analysis,Analysis Of Functional NeuroImages,Analysis of Functional NeuroImages (AFNI) is an open-source environment for processing and displaying functional MRI data—a technique for mapping human brain activity.,[0],[34]
analysis,Live Blood Analysis,"Live blood analysis (LBA), live cell analysis, Hemaview or nutritional blood analysis is the use of high-resolution dark field microscopy to observe live blood cells. Live blood analysis is promoted by some alternative medicine practitioners, who assert that it can diagnose a range of diseases. There is no scientific evidence that live blood analysis is reliable or effective, and it has been described as a fraudulent means of convincing patients that they are ill and should purchase dietary supplements.","[0, 167, 333]","[19, 186, 352]"
analysis,Recurrence Quantification Analysis,Recurrence quantification analysis (RQA) is a method of nonlinear data analysis for the investigation of dynamical systems. It quantifies the number and duration of recurrences of a dynamical system presented by its phase space trajectory.,[0],[34]
analysis,Spot Analysis,"Spot analysis, spot test analysis, or spot test is a chemical test, a simple and efficient technique where analytic assays are executed in only one, or a few drops, of a chemical solution, preferably in a great piece of filter paper, without using any sophisticated instrumentation. The development and popularization of the test is credited to Fritz Feigl.",[0],[13]
analysis,Voice Stress Analysis,"Voice stress analysis (VSA) and computer voice stress analysis (CVSA) are collectively a pseudoscientific technology that aims to infer deception from stress measured in the voice. The CVSA records the human voice using a microphone, and the technology is based on the tenet that the non-verbal, low-frequency content of the voice conveys information about the physiological and psychological state of the speaker. Typically utilized in investigative settings, the technology aims to differentiate between stressed and non-stressed outputs in response to stimuli, with high stress seen as an indication of deception.","[0, 41]","[21, 62]"
analysis,Multiple-Criteria Decision Analysis,"Multiple-criteria decision-making (MCDM) or multiple-criteria decision analysis (MCDA) is a sub-discipline of operations research that explicitly evaluates multiple conflicting criteria in decision making. Conflicting criteria are typical in evaluating options: cost or price is usually one of the main criteria, and some measure of quality is typically another criterion, easily in conflict with the cost. In purchasing a car, cost, comfort, safety, and fuel economy may be some of the main criteria we consider – it is unusual that the cheapest car is the most comfortable and the safest one. In portfolio management, managers are interested in getting high returns while simultaneously reducing risks; however, the stocks that have the potential of bringing high returns typically carry high risk of losing money. In a service industry, customer satisfaction and the cost of providing service are fundamental conflicting criteria.",[44],[79]
analysis,Structured Analysis And Design,"Structured analysis and design technique (SADT) is a systems engineering and software engineering methodology for describing systems as a hierarchy of functions. SADT is a structured analysis modelling language, which uses two types of diagrams: activity models and data models. It was developed in the late 1960s by Douglas T. Ross, and was formalized and published as IDEF0 in 1981.",[0],[30]
analysis,Climate Data Analysis Tool (CDAT),The Climate Data Analysis Tool (CDAT) is plotting software used in atmospheric sciences and climatology.,[4],[37]
machine learning,Machine Learning,"Machine learning (ML) is the study of computer algorithms that improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.","[0, 177, 355]","[16, 193, 371]"
machine learning,Machine Learning Algorithms,"Machine learning (ML) is the study of computer algorithms that improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.","[177, 355]","[204, 382]"
machine learning,Automated Machine Learning,"Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model. AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. The high degree of automation in AutoML allows non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. AutoML has been used to compare the relative importance of each factor in a prediction model.",[0],[26]
machine learning,Adversarial Machine Learning,Adversarial machine learning is a machine learning technique that attempts to fool models by supplying deceptive input. The most common reason is to cause a malfunction in a machine learning model.,[0],[28]
ML,ML.NET,"ML.NET is a free software machine learning library for the C# and F# programming languages. It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks. Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.","[0, 180]","[6, 186]"
ML,MLwiN,"MLwiN is a statistical software package for fitting multilevel models. It uses both maximum likelihood estimation and Markov chain Monte Carlo (MCMC) methods. MLwiN is based on an earlier package, MLn, but with a graphical user interface .
MLwiN represents multilevel models using mathematical notation including Greek letters and multiple subscripts, so the user needs to be familiar with such notation.","[0, 159, 240]","[5, 164, 245]"
statistic,Statistics,"Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as ""all people living in a country"" or ""every atom composing a crystal"". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.","[0, 137, 425]","[10, 147, 435]"
statistic,Statistical Software,Statistical software are specialized computer programs for analysis in statistics and econometrics.,[0],[20]
statistic,Statistical Inference,"Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.",[0],[21]
statistic,Diagnostic And Statistical Manual Of Mental Disorders,The Diagnostic and Statistical Manual of Mental Disorders is a publication by the American Psychiatric Association (APA) for the classification of mental disorders using a common language and standard criteria.,[4],[57]
statistic,Statistical Theory,"The theory of statistics provides a basis for the whole range of techniques, in both study design and data analysis, that are used within applications of statistics. The theory covers approaches to statistical-decision problems and to statistical inference, and the actions and deductions that satisfy the basic principles stated for these different approaches. Within a given approach, statistical theory gives ways of comparing statistical procedures; it can find a best possible procedure within a given context for given statistical problems, or can provide guidance on the choice between alternative procedures.",[387],[405]
statistic,Probability And Statistics,"Probability and statistics are two closely related fields in mathematics, sometimes combined for academic purposes. They are covered in several articles:Probability
Statistics
Glossary of probability and statistics
Notation in probability and statistics
Timeline of probability and statistics

","[0, 188, 227, 266]","[26, 214, 253, 292]"
statistic,Computational Statistics,"Computational statistics, or statistical computing, is the interface between statistics and computer science. It is the area of computational science specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.",[0],[24]
statistic,Descriptive Statistics,"A descriptive statistic is a summary statistic that quantitatively describes or summarizes features from a collection of information, while descriptive statistics is the process of using and analysing those statistics. Descriptive statistics is distinguished from inferential statistics by its aim to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, is not developed on the basis of probability theory, and are frequently non-parametric statistics. Even when a data analysis draws its main conclusions using inferential statistics, descriptive statistics are generally also presented. For example, in papers reporting on human subjects, typically a table is included giving the overall sample size, sample sizes in important subgroups, and demographic or clinical characteristics such as the average age, the proportion of subjects of each sex, the proportion of subjects with related co-morbidities, etc.","[140, 219, 451, 688]","[162, 241, 473, 710]"
statistic,Medical Statistics,"Medical statistics deals with applications of statistics to medicine and the health sciences, including epidemiology, public health, forensic medicine, and clinical research. Medical statistics has been a recognized branch of statistics in the United Kingdom for more than 40 years but the term has not come into general use in North America, where the wider term 'biostatistics' is more commonly used. However, ""biostatistics"" more commonly connotes all applications of statistics to biology. Medical statistics is a subdiscipline of statistics. ""It is the science of summarizing, collecting, presenting and interpreting data in medical practice, and using them to estimate the magnitude of associations and test hypotheses. It has a central role in medical investigations. It not only provides a way of organizing information on a wider and more formal basis than relying on the exchange of anecdotes and personal experience, but also takes into account the intrinsic variation inherent in most biological processes.""","[0, 175, 494]","[18, 193, 512]"
statistic,Bayesian Statistics,"Bayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability where probability expresses a degree of belief in an event. The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. This differs from a number of other interpretations of probability, such as the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials.",[0],[19]
statistic,Mathematical Statistics,"Mathematical statistics is the application of probability theory, a branch of mathematics, to statistics, as opposed to techniques for collecting statistical data. Specific mathematical techniques which are used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure theory.",[0],[23]
statistic,Statistical Learning Theory,"Statistical learning theory is a framework for machine learning
drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.","[0, 127, 226]","[27, 154, 253]"
statistic,Multivariate Statistics,Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. The application of multivariate statistics is multivariate analysis.,"[0, 164]","[23, 187]"
statistic,STATISTICA,"Statistica is an advanced analytics software package originally developed by StatSoft and currently maintained by TIBCO Software Inc.
Statistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures.","[0, 134]","[10, 144]"
statistic,Nonparametric Statistics,Nonparametric statistics is the branch of statistics that is not based solely on parametrized families of probability distributions. Nonparametric statistics is based on either being distribution-free or having a specified distribution but with the distribution's parameters unspecified. Nonparametric statistics includes both descriptive statistics and statistical inference. Nonparametric tests are often used when the assumptions of parametric tests are violated.,"[0, 133, 288]","[24, 157, 312]"
statistic,Statistical Thinking,"Statistical thinking is one of the tools for process analysis. Statistical thinking relates processes and statistics, and is based on the following principles:All work occurs in a system of interconnected processes.
Variation exists in all processes
Understanding and reducing variation are keys to success.","[0, 63]","[20, 83]"
statistic,Statistical Language Acquisition,"Statistical language acquisition, a branch of developmental psycholinguistics, studies the process by which humans develop the ability to perceive, produce, comprehend, and communicate with natural language in all of its aspects through the use of general learning mechanisms operating on statistical patterns in the linguistic input. Statistical learning acquisition claims that infants language learning is based on pattern perception rather than an innate biological grammar. Several statistical elements such as frequency of words, frequent frames, phonotactic patterns and other regularities provide information on language structure and meaning for facilitation of language acquisition.",[0],[32]
statistic,Economic Statistics,"Economic statistics is a topic in applied statistics that concerns the collection, processing, compilation, dissemination, and analysis of economic data. It is also common to call the data themselves 'economic statistics', but for this usage see economic data. The data of concern to economic statistics may include those of an economy within a region, country, or group of countries. Economic statistics may also refer to a subtopic of official statistics for data produced by official organizations. Analyses within economic statistics both make use of and provide the empirical data needed in economic research, whether descriptive or econometric. They are a key input for decision making as to economic policy.
The subject includes statistical analysis of topics and problems in microeconomics, macroeconomics, business, finance, forecasting, data quality, and policy evaluation. It also includes such considerations as what data to collect in order to quantify some particular aspect of an economy and of how best to collect in any given instance.","[0, 201, 284, 385, 518]","[19, 220, 303, 404, 537]"
statistic,Statistical Graphics,"Statistical graphics, also known as statistical graphical techniques, are graphics used in the field of statistics for data visualization.",[0],[20]
statistic,Statistical Arbitrage,"In finance, statistical arbitrage is a class of short-term financial trading strategies that employ mean reversion models involving broadly diversified portfolios of securities held for short periods of time. These strategies are supported by substantial mathematical, computational, and trading platforms.",[12],[33]
statistic,Statistical Physics,"Statistical physics is a branch of physics that evolved from a foundation of statistical mechanics, which uses methods of probability theory and statistics, and particularly the mathematical tools for dealing with large populations and approximations, in solving physical problems. It can describe a wide variety of fields with an inherently stochastic nature. Its applications include many problems in the fields of physics, biology, chemistry, neuroscience. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.",[0],[19]
statistic,Parametric Statistics,"
Parametric statistics is a branch of statistics which assumes that sample data comes from a population that can be adequately modeled by a probability distribution that has a fixed set of parameters. Conversely a non-parametric model differs precisely in that it makes no assumptions about a parametric distribution when modeling the data.",[1],[22]
statistic,Social Statistics,"Social statistics is the use of statistical measurement systems to study human behavior in a social environment. This can be accomplished through polling a group of people, evaluating a subset of data obtained about a group of people, or by observation and statistical analysis of a set of data that relates to people and their behaviors.",[0],[17]
statistic,Robust Statistics,"Robust statistics are statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. Robust statistical methods have been developed for many common problems, such as estimating location, scale, and regression parameters. One motivation is to produce statistical methods that are not unduly affected by outliers. Another motivation is to provide methods with good performance when there are small departures from parametric distribution. For example, robust methods work well for mixtures of two normal distributions with different standard-deviations; under this model, non-robust methods like a t-test work poorly.",[0],[17]
statistic,Statistical Parametric Mapping,Statistical parametric mapping (SPM) is a statistical technique for examining differences in brain activity recorded during functional neuroimaging experiments. It was created by Karl Friston. It may alternatively refer to software created by the Wellcome Department of Imaging Neuroscience at University College London to carry out such analyses.,[0],[30]
statistic,Statistical Coupling Analysis,"Statistical coupling analysis or SCA is a technique used in bioinformatics to measure covariation between pairs of amino acids in a protein multiple sequence alignment (MSA). More specifically, it quantifies how much the amino acid distribution at some position i changes upon a perturbation of the amino acid distribution at another position j. The resulting statistical coupling energy indicates the degree of evolutionary dependence between the residues, with higher coupling energy corresponding to increased dependence.",[0],[29]
statistic,National Vital Statistics System,"The National Vital Statistics System (NVSS) is an inter-governmental system of sharing data on the vital statistics of the population of the United States. It involves coordination between the different state health departments of the US states and the National Center for Health Statistics, a division of the Centers for Disease Control and Prevention.",[4],[36]
